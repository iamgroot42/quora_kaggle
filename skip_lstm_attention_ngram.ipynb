{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings', 'train.csv', 'test.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "base_dir = \"./input\"\n",
    "print(os.listdir(base_dir))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "e31d6e126881ee56a1de3efe02fcf309e900ef00"
   },
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "maxlen = 75 # max number of words in a question to use\n",
    "max_features = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "522d9790478f62193ea5c315372a2ab9cbe9b27f"
   },
   "source": [
    "**Load packages and data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, GRU, LSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import Callback\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from gensim.models import KeyedVectors as kv\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cores: 32\n",
      "time: 37.2 ms\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "from multiprocessing import Pool\n",
    "\n",
    "num_partitions = 20  # number of partitions to split dataframe\n",
    "num_cores = psutil.cpu_count()  # number of cores on your machine\n",
    "\n",
    "print('number of cores:', num_cores)\n",
    "def df_parallelize_run(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 903 ms\n"
     ]
    }
   ],
   "source": [
    "# Don't hog GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.19 ms\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5cdc95950037613c690c49b27930ae0f59eb23c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.3 ms\n"
     ]
    }
   ],
   "source": [
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(base_dir + \"/train.csv\")\n",
    "    test_df = pd.read_csv(base_dir + \"/test.csv\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dba1893c267a1e7536bbf720636647d85c7e349c"
   },
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_and_prec()\n",
    "train_y = train_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create metadata features, possibly not captured by non-neural/neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.42 ms\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def count_punct(text):\n",
    "    regular_punct = list(string.punctuation)\n",
    "    extra_punct = [\n",
    "        ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "        '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "        '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "        '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "        '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "        '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "        '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "        'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "        '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "        '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "    all_punct = list(set(regular_punct + extra_punct))\n",
    "    count = 0\n",
    "    for punct in all_punct:\n",
    "        count += text.count(punct)\n",
    "    return count\n",
    "    \n",
    "def add_meta_features(df):\n",
    "    # Number of characters\n",
    "    df[\"meta_1\"] = df[\"question_text\"].apply(lambda text: len(text))\n",
    "    # Number of words\n",
    "    df[\"meta_2\"] = df[\"question_text\"].apply(lambda text: len(text.split(' ')))\n",
    "    # Number of uppercase characters\n",
    "    df[\"meta_3\"] = df[\"question_text\"].apply(lambda text: sum(1 for c in text if c.isupper()))\n",
    "    # Number of digits\n",
    "    df[\"meta_4\"] = df[\"question_text\"].apply(lambda text: sum(1 for c in text if c.isdigit()))\n",
    "    # Number of non-alphanumeric characters\n",
    "    df[\"meta_5\"] = df[\"question_text\"].apply(lambda text: sum(1 for c in text if not c.isalpha()))\n",
    "    # Number of question marks\n",
    "    df[\"meta_6\"] = df[\"question_text\"].apply(lambda text: text.count('?'))\n",
    "    # Average word length\n",
    "    df[\"meta_7\"] = df[\"question_text\"].apply(lambda text: sum(len(word) for word in text.split(' ')) / len(text.split(' ')))\n",
    "    # Presence of mathematical latex equations\n",
    "    df[\"meta_7\"] = df[\"question_text\"].apply(lambda text: text.count(\"[math]\"))\n",
    "    # Punctuation\n",
    "    df[\"meta_8\"] = df[\"question_text\"].apply(lambda text: count_punct(text))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "train_df = df_parallelize_run(train_df, add_meta_features)\n",
    "test_df = df_parallelize_run(test_df, add_meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.55 ms\n"
     ]
    }
   ],
   "source": [
    "def create_metafeatures(df):\n",
    "    metadata = []\n",
    "    for i in range(len(df[\"question_text\"])):\n",
    "        metadata.append([ df[key][i] for key in df.keys() if \"meta_\" in key])\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate n-gram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit tfidf word model\n",
      "time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', analyzer='word',\n",
    "                                   max_df=0.3, min_df=0.0001, ngram_range=(1,3),\n",
    "                                   lowercase=False,\n",
    "                                   strip_accents='unicode', sublinear_tf=True)\n",
    "\n",
    "tfidf_vectorizer.fit(train_df[\"question_text\"])\n",
    "print(\"Fit tfidf word model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10110 tfidf-word features\n",
      "time: 76.1 ms\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_vectorizer.get_feature_names()), \"tfidf-word features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 57.3 s\n"
     ]
    }
   ],
   "source": [
    "tfidf_trainX = tfidf_vectorizer.transform(train_df['question_text']).toarray()\n",
    "tfidf_testX = tfidf_vectorizer.transform(test_df['question_text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "# Clear RAM\n",
    "del tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-1 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13 ms\n"
     ]
    }
   ],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cyclic LR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.6 ms\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "# Create stratified train-val sets (to ensure class balance in these two)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=50)\n",
    "train_indices, val_indices = list(skf.split(tfidf_trainX, train_y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "tfidf_trainx, tfidf_trainy = tfidf_trainX[train_indices], train_y[train_indices]\n",
    "tfidf_valx, tfidf_valy = tfidf_trainX[val_indices], train_y[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "# Free RAM\n",
    "del tfidf_trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5min 19s\n"
     ]
    }
   ],
   "source": [
    "train_metadata = create_metafeatures(train_df)\n",
    "test_metadata = create_metafeatures(test_df)\n",
    "\n",
    "train_metadata = np.array(train_metadata)\n",
    "test_metadata = np.array(test_metadata)\n",
    "\n",
    "train_metadata, val_metadata = train_metadata[train_indices], train_metadata[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paragag/persona/lib/python3.5/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06726221755123489"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.96 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "meta_clf = RandomForestClassifier(n_estimators=10, random_state=2019, n_jobs=-1)\n",
    "meta_clf.fit(train_metadata, tfidf_trainy)\n",
    "f1_score(tfidf_trainy, meta_clf.predict(train_metadata))\n",
    "f1_score(tfidf_valy, meta_clf.predict(val_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.02 ms\n"
     ]
    }
   ],
   "source": [
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1279998 samples, validate on 26124 samples\n",
      "Epoch 1/5\n",
      "1279998/1279998 [==============================] - 200s 156us/step - loss: 0.0404 - f1: 0.4185 - val_loss: 0.0255 - val_f1: 0.5458\n",
      "Epoch 2/5\n",
      "1279998/1279998 [==============================] - 192s 150us/step - loss: 0.0177 - f1: 0.5481 - val_loss: 0.0180 - val_f1: 0.5454\n",
      "Epoch 3/5\n",
      "1279998/1279998 [==============================] - 194s 151us/step - loss: 0.0155 - f1: 0.5829 - val_loss: 0.0173 - val_f1: 0.5282\n",
      "Epoch 4/5\n",
      "1279998/1279998 [==============================] - 193s 151us/step - loss: 0.0144 - f1: 0.6021 - val_loss: 0.0179 - val_f1: 0.5079\n",
      "Epoch 5/5\n",
      "1279998/1279998 [==============================] - 195s 152us/step - loss: 0.0137 - f1: 0.6207 - val_loss: 0.0179 - val_f1: 0.5258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f87565cf8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16min 15s\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "\n",
    "def meta_model(fet_size):\n",
    "    inp = Input(shape=(fet_size,))\n",
    "\n",
    "    conc = Dense(64)(inp)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = Activation(\"relu\")(conc)\n",
    "\n",
    "    conc = Dense(32)(inp)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = Activation(\"relu\")(conc)\n",
    "    \n",
    "    conc = Dense(16)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = Activation(\"relu\")(conc)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer=Adam(0.001), metrics=[f1])\n",
    "    model.compile(loss=focal_loss(alpha=0.5), optimizer=Adam(0.001), metrics=[f1])\n",
    "    return model\n",
    "\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "               step_size=3000, mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "ngram_model = meta_model(tfidf_trainx.shape[1])\n",
    "ngram_model.fit(tfidf_trainx, tfidf_trainy,\n",
    "                validation_data=(tfidf_valx, tfidf_valy),\n",
    "                epochs=5, batch_size=8192, callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5196691547306839\n",
      "time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ngram_threshold = 0.3\n",
    "print(f1_score(tfidf_valy, ngram_model.predict(tfidf_valx, batch_size=16384) > ngram_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.36 ms\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        if word in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.capitalize() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.capitalize()]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.lower() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.lower()]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.upper() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.upper()]\n",
    "            nb_known_words += vocab[word]\n",
    "        else:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.76 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train_df.drop('target', axis=1), test_df]).reset_index(drop=True)\n",
    "vocab = build_vocab(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.71 ms\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float16')\n",
    "    \n",
    "    if file == base_dir + '/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    elif file == base_dir + '/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin':\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "embed_glove = load_embed(base_dir + '/embeddings/glove.840B.300d/glove.840B.300d.txt')\n",
    "embed_wiki = load_embed(base_dir + '/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec')\n",
    "embed_google = load_embed(base_dir + '/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 33.92% of vocab\n",
      "Found embeddings for  88.20% of all text\n",
      "Found embeddings for 31.63% of vocab\n",
      "Found embeddings for  87.74% of all text\n",
      "Found embeddings for 26.24% of vocab\n",
      "Found embeddings for  87.26% of all text\n",
      "time: 5.71 s\n"
     ]
    }
   ],
   "source": [
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "oov_glove = {\"oov_rate\": len(oov_glove) / len(vocab), 'oov_words': oov_glove}\n",
    "\n",
    "oov_wiki = check_coverage(vocab, embed_wiki)\n",
    "oov_wiki = {\"oov_rate\": len(oov_wiki) / len(vocab), 'oov_words': oov_wiki}\n",
    "\n",
    "oov_google = check_coverage(vocab, embed_google)\n",
    "oov_google = {\"oov_rate\": len(oov_google) / len(vocab), 'oov_words': oov_google}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.2 ms\n"
     ]
    }
   ],
   "source": [
    "# remove space\n",
    "import re\n",
    "\n",
    "spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n",
    "def remove_space(text):\n",
    "    \"\"\"\n",
    "    remove extra spaces and ending space if any\n",
    "    \"\"\"\n",
    "    for space in spaces:\n",
    "        text = text.replace(space, ' ')\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.56 ms\n"
     ]
    }
   ],
   "source": [
    "# replace strange punctuations and raplace diacritics\n",
    "from unicodedata import category, name, normalize\n",
    "\n",
    "def remove_diacritics(s):\n",
    "    return ''.join(c for c in normalize('NFKD', s.replace('ø', 'o').replace('Ø', 'O').replace('⁻', '-').replace('₋', '-'))\n",
    "                  if category(c) != 'Mn')\n",
    "\n",
    "special_punc_mappings = {\"—\": \"-\", \"–\": \"-\", \"_\": \"-\", '”': '\"', \"″\": '\"', '“': '\"', '•': '.', '−': '-',\n",
    "                         \"’\": \"'\", \"‘\": \"'\", \"´\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','،':'','„':'',\n",
    "                         '…': ' ... ', '\\ufeff': ''}\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    # 注意顺序，remove_diacritics放前面会导致 'don´t' 被处理为 'don t'\n",
    "    text = remove_diacritics(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.72 ms\n"
     ]
    }
   ],
   "source": [
    "# clean numbers\n",
    "def clean_number(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.98 ms\n"
     ]
    }
   ],
   "source": [
    "rare_words_mapping = {' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA',\n",
    "                      'u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n",
    "                      ' U.s ': 'USA', ' u.S ': ' USA ', 'fu.k': 'fuck', 'U.K.': 'UK', ' u.k ': ' UK ',\n",
    "                      ' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n",
    "                      'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n",
    "                      'Ra apist': 'Rapist', '2fifth': 'twenty fifth', '2third': 'twenty third',\n",
    "                      '2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n",
    "                      'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n",
    "                      'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin',\n",
    "                      'fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n",
    "                      'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n",
    "                      'culturr': 'culture',\n",
    "                      'weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n",
    "                      'indans': 'indians', 'mastuburate': 'masturbate', 'f**k': 'fuck', 'F**k': 'fuck', 'F**K': 'fuck',\n",
    "                      ' u r ': ' you are ', ' u ': ' you ', '操你妈': 'fuck your mother', 'e.g.': 'for example',\n",
    "                      'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n",
    "                      'f***': 'fuck', 'f**': 'fuc', 'F***': 'fuck', 'F**': 'fuc',\n",
    "                      'a****': 'assho', 'a**': 'ass', 'h***': 'hole', 'A****': 'assho', 'A**': 'ass', 'H***': 'hole',\n",
    "                      's***': 'shit', 's**': 'shi', 'S***': 'shit', 'S**': 'shi', 'Sh**': 'shit',\n",
    "                      'p****': 'pussy', 'p*ssy': 'pussy', 'P****': 'pussy',\n",
    "                      'p***': 'porn', 'p*rn': 'porn', 'P***': 'porn',\n",
    "                      'st*up*id': 'stupid',\n",
    "                      'd***': 'dick', 'di**': 'dick', 'h*ck': 'hack',\n",
    "                      'b*tch': 'bitch', 'bi*ch': 'bitch', 'bit*h': 'bitch', 'bitc*': 'bitch', 'b****': 'bitch',\n",
    "                      'b***': 'bitch', 'b**': 'bit', 'b*ll': 'bull'\n",
    "                      }\n",
    "\n",
    "\n",
    "def pre_clean_rare_words(text):\n",
    "    for rare_word in rare_words_mapping:\n",
    "        if rare_word in text:\n",
    "            text = text.replace(rare_word, rare_words_mapping[rare_word])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.91 ms\n"
     ]
    }
   ],
   "source": [
    "# de-contract the contraction\n",
    "def decontracted(text):\n",
    "    # specific\n",
    "    text = re.sub(r\"(W|w)on(\\'|\\’)t \", \"will not \", text)\n",
    "    text = re.sub(r\"(C|c)an(\\'|\\’)t \", \"can not \", text)\n",
    "    text = re.sub(r\"(Y|y)(\\'|\\’)all \", \"you all \", text)\n",
    "    text = re.sub(r\"(Y|y)a(\\'|\\’)ll \", \"you all \", text)\n",
    "\n",
    "    # general\n",
    "    text = re.sub(r\"(I|i)(\\'|\\’)m \", \"i am \", text)\n",
    "    text = re.sub(r\"(A|a)in(\\'|\\’)t \", \"is not \", text)\n",
    "    text = re.sub(r\"n(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)re \", \" are \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)s \", \" is \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)d \", \" would \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ll \", \" will \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)t \", \" not \", text)\n",
    "    text = re.sub(r\"(\\'|\\’)ve \", \" have \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.18 ms\n"
     ]
    }
   ],
   "source": [
    "def clean_latex(text):\n",
    "    \"\"\"\n",
    "    convert r\"[math]\\vec{x} + \\vec{y}\" to English\n",
    "    \"\"\"\n",
    "    # edge case\n",
    "    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\\\', ' LaTex ', text)\n",
    "\n",
    "    pattern_to_sub = {\n",
    "        r'\\\\mathrm': ' LaTex math mode ',\n",
    "        r'\\\\mathbb': ' LaTex math mode ',\n",
    "        r'\\\\boxed': ' LaTex equation ',\n",
    "        r'\\\\begin': ' LaTex equation ',\n",
    "        r'\\\\end': ' LaTex equation ',\n",
    "        r'\\\\left': ' LaTex equation ',\n",
    "        r'\\\\right': ' LaTex equation ',\n",
    "        r'\\\\(over|under)brace': ' LaTex equation ',\n",
    "        r'\\\\text': ' LaTex equation ',\n",
    "        r'\\\\vec': ' vector ',\n",
    "        r'\\\\var': ' variable ',\n",
    "        r'\\\\theta': ' theta ',\n",
    "        r'\\\\mu': ' average ',\n",
    "        r'\\\\min': ' minimum ',\n",
    "        r'\\\\max': ' maximum ',\n",
    "        r'\\\\sum': ' + ',\n",
    "        r'\\\\times': ' * ',\n",
    "        r'\\\\cdot': ' * ',\n",
    "        r'\\\\hat': ' ^ ',\n",
    "        r'\\\\frac': ' / ',\n",
    "        r'\\\\div': ' / ',\n",
    "        r'\\\\sin': ' Sine ',\n",
    "        r'\\\\cos': ' Cosine ',\n",
    "        r'\\\\tan': ' Tangent ',\n",
    "        r'\\\\infty': ' infinity ',\n",
    "        r'\\\\int': ' integer ',\n",
    "        r'\\\\in': ' in ',\n",
    "    }\n",
    "    # post process for look up\n",
    "    pattern_dict = {k.strip('\\\\'): v for k, v in pattern_to_sub.items()}\n",
    "    # init re\n",
    "    patterns = pattern_to_sub.keys()\n",
    "    pattern_re = re.compile('(%s)' % '|'.join(patterns))\n",
    "\n",
    "    def _replace(match):\n",
    "        \"\"\"\n",
    "        reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa\n",
    "        \"\"\"\n",
    "        try:\n",
    "            word = pattern_dict.get(match.group(0).strip('\\\\'))\n",
    "        except KeyError:\n",
    "            word = match.group(0)\n",
    "        return word\n",
    "    return pattern_re.sub(_replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.1 ms\n"
     ]
    }
   ],
   "source": [
    "misspell_mapping = {'Terroristan': 'terrorist Pakistan', 'terroristan': 'terrorist Pakistan',\n",
    "                    'FATF': 'Western summit conference',\n",
    "                    'BIMARU': 'BIMARU Bihar, Madhya Pradesh, Rajasthan, Uttar Pradesh', 'Hinduphobic': 'Hindu phobic',\n",
    "                    'hinduphobic': 'Hindu phobic', 'Hinduphobia': 'Hindu phobic', 'hinduphobia': 'Hindu phobic',\n",
    "                    'Babchenko': 'Arkady Arkadyevich Babchenko faked death', 'Boshniaks': 'Bosniaks',\n",
    "                    'Dravidanadu': 'Dravida Nadu', 'mysoginists': 'misogynists', 'MGTOWS': 'Men Going Their Own Way',\n",
    "                    'mongloid': 'Mongoloid', 'unsincere': 'insincere', 'meninism': 'male feminism',\n",
    "                    'jewplicate': 'jewish replicate', 'jewplicates': 'jewish replicate', 'andhbhakts': 'and Bhakt',\n",
    "                    'unoin': 'Union', 'daesh': 'Islamic State of Iraq and the Levant', 'burnol': 'movement about Modi',\n",
    "                    'Kalergi': 'Coudenhove-Kalergi', 'Bhakts': 'Bhakt', 'bhakts': 'Bhakt', 'Tambrahms': 'Tamil Brahmin',\n",
    "                    'Pahul': 'Amrit Sanskar', 'SJW': 'social justice warrior', 'SJWs': 'social justice warrior',\n",
    "                    ' incel': ' involuntary celibates', ' incels': ' involuntary celibates', 'emiratis': 'Emiratis',\n",
    "                    'weatern': 'western', 'westernise': 'westernize', 'Pizzagate': 'debunked conspiracy theory',\n",
    "                    'naïve': 'naive', 'Skripal': 'Russian military officer', 'Skripals': 'Russian military officer',\n",
    "                    'Remainers': 'British remainer', 'Novichok': 'Soviet Union agents',\n",
    "                    'gauri lankesh': 'Famous Indian Journalist', 'Castroists': 'Castro supporters',\n",
    "                    'remainers': 'British remainer', 'bremainer': 'British remainer', 'antibrahmin': 'anti Brahminism',\n",
    "                    'HYPSM': ' Harvard, Yale, Princeton, Stanford, MIT', 'HYPS': ' Harvard, Yale, Princeton, Stanford',\n",
    "                    'kompromat': 'compromising material', 'Tharki': 'pervert', 'tharki': 'pervert',\n",
    "                    'mastuburate': 'masturbate', 'Zoë': 'Zoe', 'indans': 'Indian', ' xender': ' gender',\n",
    "                    'Naxali ': 'Naxalite ', 'Naxalities': 'Naxalites', 'Bathla': 'Namit Bathla',\n",
    "                    'Mewani': 'Indian politician Jignesh Mevani', 'Wjy': 'Why',\n",
    "                    'Fadnavis': 'Indian politician Devendra Fadnavis', 'Awadesh': 'Indian engineer Awdhesh Singh',\n",
    "                    'Awdhesh': 'Indian engineer Awdhesh Singh', 'Khalistanis': 'Sikh separatist movement',\n",
    "                    'madheshi': 'Madheshi', 'BNBR': 'Be Nice, Be Respectful',\n",
    "                    'Jair Bolsonaro': 'Brazilian President politician', 'XXXTentacion': 'Tentacion',\n",
    "                    'Slavoj Zizek': 'Slovenian philosopher',\n",
    "                    'borderliners': 'borderlines', 'Brexit': 'British Exit', 'Brexiter': 'British Exit supporter',\n",
    "                    'Brexiters': 'British Exit supporters', 'Brexiteer': 'British Exit supporter',\n",
    "                    'Brexiteers': 'British Exit supporters', 'Brexiting': 'British Exit',\n",
    "                    'Brexitosis': 'British Exit disorder', 'brexit': 'British Exit',\n",
    "                    'brexiters': 'British Exit supporters', 'jallikattu': 'Jallikattu', 'fortnite': 'Fortnite',\n",
    "                    'Swachh': 'Swachh Bharat mission campaign ', 'Quorans': 'Quora users', 'Qoura': 'Quora',\n",
    "                    'quoras': 'Quora', 'Quroa': 'Quora', 'QUORA': 'Quora', 'Stupead': 'stupid',\n",
    "                    'narcissit': 'narcissist', 'trigger nometry': 'trigonometry',\n",
    "                    'trigglypuff': 'student Criticism of Conservatives', 'peoplelook': 'people look',\n",
    "                    'paedophelia': 'paedophilia', 'Uogi': 'Yogi', 'adityanath': 'Adityanath',\n",
    "                    'Yogi Adityanath': 'Indian monk and Hindu nationalist politician',\n",
    "                    'Awdhesh Singh': 'Commissioner of India', 'Doklam': 'Tibet', 'Drumpf ': 'Donald Trump fool ',\n",
    "                    'Drumpfs': 'Donald Trump fools', 'Strzok': 'Hillary Clinton scandal', 'rohingya': 'Rohingya ',\n",
    "                    ' wumao ': ' cheap Chinese stuff ', 'wumaos': 'cheap Chinese stuff', 'Sanghis': 'Sanghi',\n",
    "                    'Tamilans': 'Tamils', 'biharis': 'Biharis', 'Rejuvalex': 'hair growth formula Medicine',\n",
    "                    'Fekuchand': 'PM Narendra Modi in India', 'feku': 'Feku', 'Chaiwala': 'tea seller in India',\n",
    "                    'Feku': 'PM Narendra Modi in India ', 'deplorables': 'deplorable', 'muhajirs': 'Muslim immigrant',\n",
    "                    'Gujratis': 'Gujarati', 'Chutiya': 'Tibet people ', 'Chutiyas': 'Tibet people ',\n",
    "                    'thighing': 'masterbate between the legs of a female infant', '卐': 'Nazi Germany',\n",
    "                    'Pribumi': 'Native Indonesian', 'Gurmehar': 'Gurmehar Kaur Indian student activist',\n",
    "                    'Khazari': 'Khazars', 'Demonetization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                    'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                    'antinationals': 'antinational', 'Cryptocurrencies': 'cryptocurrency',\n",
    "                    'cryptocurrencies': 'cryptocurrency', 'Hindians': 'North Indian', 'Hindian': 'North Indian',\n",
    "                    'vaxxer': 'vocal nationalist ', 'remoaner': 'remainer ', 'bremoaner': 'British remainer ',\n",
    "                    'Jewism': 'Judaism', 'Eroupian': 'European', \"J & K Dy CM H ' ble Kavinderji\": '',\n",
    "                    'WMAF': 'White male married Asian female', 'AMWF': 'Asian male married White female',\n",
    "                    'moeslim': 'Muslim', 'cishet': 'cisgender and heterosexual person', 'Eurocentrics': 'Eurocentrism',\n",
    "                    'Eurocentric': 'Eurocentrism', 'Afrocentrics': 'Africa centrism', 'Afrocentric': 'Africa centrism',\n",
    "                    'Jewdar': 'Jew dar', 'marathis': 'Marathi', 'Gynophobic': 'Gyno phobic',\n",
    "                    'Trumpanzees': 'Trump chimpanzee fool', 'Crimean': 'Crimea people ', 'atrracted': 'attract',\n",
    "                    'Myeshia': 'widow of Green Beret killed in Niger', 'demcoratic': 'Democratic', 'raaping': 'raping',\n",
    "                    'feminazism': 'feminism nazi', 'langague': 'language', 'sathyaraj': 'actor',\n",
    "                    'Hongkongese': 'HongKong people', 'hongkongese': 'HongKong people', 'Kashmirians': 'Kashmirian',\n",
    "                    'Chodu': 'fucker', 'penish': 'penis',\n",
    "                    'chitpavan konkanastha': 'Hindu Maharashtrian Brahmin community',\n",
    "                    'Madridiots': 'Real Madrid idiot supporters', 'Ambedkarite': 'Dalit Buddhist movement ',\n",
    "                    'ReleaseTheMemo': 'cry for the right and Trump supporters', 'harrase': 'harass',\n",
    "                    'Barracoon': 'Black slave', 'Castrater': 'castration', 'castrater': 'castration',\n",
    "                    'Rapistan': 'Pakistan rapist', 'rapistan': 'Pakistan rapist', 'Turkified': 'Turkification',\n",
    "                    'turkified': 'Turkification', 'Dumbassistan': 'dumb ass Pakistan', 'facetards': 'Facebook retards',\n",
    "                    'rapefugees': 'rapist refugee', 'Khortha': 'language in the Indian state of Jharkhand',\n",
    "                    'Magahi': 'language in the northeastern Indian', 'Bajjika': 'language spoken in eastern India',\n",
    "                    'superficious': 'superficial', 'Sense8': 'American science fiction drama web television series',\n",
    "                    'Saipul Jamil': 'Indonesia artist', 'bhakht': 'bhakti', 'Smartia': 'dumb nation',\n",
    "                    'absorve': 'absolve', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Whta': 'What',\n",
    "                    'esspecial': 'especial', 'doI': 'do I', 'theBest': 'the best',\n",
    "                    'howdoes': 'how does', 'Etherium': 'Ethereum', '2k17': '2017', '2k18': '2018', 'qiblas': 'qibla',\n",
    "                    'Hello4 2 cab': 'Online Cab Booking', 'bodyshame': 'body shaming', 'bodyshoppers': 'body shopping',\n",
    "                    'bodycams': 'body cams', 'Cananybody': 'Can any body', 'deadbody': 'dead body',\n",
    "                    'deaddict': 'de addict', 'Northindian': 'North Indian ', 'northindian': 'north Indian ',\n",
    "                    'northkorea': 'North Korea', 'koreaboo': 'Korea boo ',\n",
    "                    'Brexshit': 'British Exit bullshit', 'shitpost': 'shit post', 'shitslam': 'shit Islam',\n",
    "                    'shitlords': 'shit lords', 'Fck': 'Fuck', 'Clickbait': 'click bait ', 'clickbait': 'click bait ',\n",
    "                    'mailbait': 'mail bait', 'healhtcare': 'healthcare', 'trollbots': 'troll bots',\n",
    "                    'trollled': 'trolled', 'trollimg': 'trolling', 'cybertrolling': 'cyber trolling',\n",
    "                    'sickular': 'India sick secular ', 'Idiotism': 'idiotism',\n",
    "                    'Niggerism': 'Nigger', 'Niggeriah': 'Nigger'}\n",
    "\n",
    "def clean_misspell(text):\n",
    "    for bad_word in misspell_mapping:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, misspell_mapping[bad_word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.9 ms\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "regular_punct = list(string.punctuation)\n",
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
    "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
    "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
    "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
    "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
    "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
    "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']\n",
    "all_punct = list(set(regular_punct + extra_punct))\n",
    "# do not spacing - and .\n",
    "all_punct.remove('-')\n",
    "all_punct.remove('.')\n",
    "\n",
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, \" \" + punc + \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 85.6 ms\n"
     ]
    }
   ],
   "source": [
    "# spell check and according to bad case analyse\n",
    "bad_case_words = {'jewprofits': 'jew profits', 'QMAS': 'Quality Migrant Admission Scheme', 'casterating': 'castrating',\n",
    "                  'Kashmiristan': 'Kashmir', 'CareOnGo': 'India first and largest Online distributor of medicines',\n",
    "                  'Setya Novanto': 'a former Indonesian politician', 'TestoUltra': 'male sexual enhancement supplement',\n",
    "                  'rammayana': 'ramayana', 'Badaganadu': 'Brahmin community that mainly reside in Karnataka',\n",
    "                  'bitcjes': 'bitches', 'mastubrate': 'masturbate', 'Français': 'France',\n",
    "                  'Adsresses': 'address', 'flemmings': 'flemming', 'intermate': 'inter mating', 'feminisam': 'feminism',\n",
    "                  'cuckholdry': 'cuckold', 'Niggor': 'black hip-hop and electronic artist', 'narcsissist': 'narcissist',\n",
    "                  'Genderfluid': 'Gender fluid', ' Im ': ' I am ', ' dont ': ' do not ', 'Qoura': 'Quora',\n",
    "                  'ethethnicitesnicites': 'ethnicity', 'Namit Bathla': 'Content Writer', 'What sApp': 'WhatsApp',\n",
    "                  'Führer': 'Fuhrer', 'covfefe': 'coverage', 'accedentitly': 'accidentally', 'Cuckerberg': 'Zuckerberg',\n",
    "                  'transtrenders': 'incredibly disrespectful to real transgender people',\n",
    "                  'frozen tamod': 'Pornographic website', 'hindians': 'North Indian', 'hindian': 'North Indian',\n",
    "                  'celibatess': 'celibates', 'Trimp': 'Trump', 'wanket': 'wanker', 'wouldd': 'would',\n",
    "                  'arragent': 'arrogant', 'Ra - apist': 'rapist', 'idoot': 'idiot', 'gangstalkers': 'gangs talkers',\n",
    "                  'toastsexual': 'toast sexual', 'inapropriately': 'inappropriately', 'dumbassess': 'dumbass',\n",
    "                  'germanized': 'become german', 'helisexual': 'sexual', 'regilious': 'religious',\n",
    "                  'timetraveller': 'time traveller', 'darkwebcrawler': 'dark webcrawler', 'routez': 'route',\n",
    "                  'trumpians': 'Trump supporters', 'irreputable': 'reputation', 'serieusly': 'seriously',\n",
    "                  'anti cipation': 'anticipation', 'microaggression': 'micro aggression', 'Afircans': 'Africans',\n",
    "                  'microapologize': 'micro apologize', 'Vishnus': 'Vishnu', 'excritment': 'excitement',\n",
    "                  'disagreemen': 'disagreement', 'gujratis': 'gujarati', 'gujaratis': 'gujarati',\n",
    "                  'ugggggggllly': 'ugly',\n",
    "                  'Germanity': 'German', 'SoyBoys': 'cuck men lacking masculine characteristics',\n",
    "                  'н': 'h', 'м': 'm', 'ѕ': 's', 'т': 't', 'в': 'b', 'υ': 'u', 'ι': 'i',\n",
    "                  'genetilia': 'genitalia', 'r - apist': 'rapist', 'Borokabama': 'Barack Obama',\n",
    "                  'arectifier': 'rectifier', 'pettypotus': 'petty potus', 'magibabble': 'magi babble',\n",
    "                  'nothinking': 'thinking', 'centimiters': 'centimeters', 'saffronized': 'India, politics, derogatory',\n",
    "                  'saffronize': 'India, politics, derogatory', ' incect ': ' insect ', 'weenus': 'elbow skin',\n",
    "                  'Pakistainies': 'Pakistanis', 'goodspeaks': 'good speaks', 'inpregnated': 'in pregnant',\n",
    "                  'rapefilms': 'rape films', 'rapiest': 'rapist', 'hatrednesss': 'hatred',\n",
    "                  'heightism': 'height discrimination', 'getmy': 'get my', 'onsocial': 'on social',\n",
    "                  'worstplatform': 'worst platform', 'platfrom': 'platform', 'instagate': 'instigate',\n",
    "                  'Loy Machedeo': 'person', ' dsire ': ' desire ', 'iservant': 'servant', 'intelliegent': 'intelligent',\n",
    "                  'WW 1': ' WW1 ', 'WW 2': ' WW2 ', 'ww 1': ' WW1 ', 'ww 2': ' WW2 ',\n",
    "                  'keralapeoples': 'kerala peoples', 'trumpervotes': 'trumper votes', 'fucktrumpet': 'fuck trumpet',\n",
    "                  'likebJaish': 'like bJaish', 'likemy': 'like my', 'Howlikely': 'How likely',\n",
    "                  'disagreementts': 'disagreements', 'disagreementt': 'disagreement',\n",
    "                  'meninist': \"male chauvinism\", 'feminists': 'feminism supporters', 'Ghumendra': 'Bhupendra',\n",
    "                  'emellishments': 'embellishments',\n",
    "                  'settelemen': 'settlement',\n",
    "                  'Richmencupid': 'rich men dating website', 'richmencupid': 'rich men dating website',\n",
    "                  'Gaudry - Schost': '', 'ladymen': 'ladyboy', 'hasserment': 'Harassment',\n",
    "                  'instrumentalizing': 'instrument', 'darskin': 'dark skin', 'balckwemen': 'balck women',\n",
    "                  'recommendor': 'recommender', 'wowmen': 'women', 'expertthink': 'expert think',\n",
    "                  'whitesplaining': 'white splaining', 'Inquoraing': 'inquiring', 'whilemany': 'while many',\n",
    "                  'manyother': 'many other', 'involvedinthe': 'involved in the', 'slavetrade': 'slave trade',\n",
    "                  'aswell': 'as well', 'fewshowanyRemorse': 'few show any Remorse', 'trageting': 'targeting',\n",
    "                  'getile': 'gentile', 'Gujjus': 'derogatory Gujarati', 'judisciously': 'judiciously',\n",
    "                  'Hue Mungus': 'feminist bait', 'Hugh Mungus': 'feminist bait', 'Hindustanis': '',\n",
    "                  'Virushka': 'Great Relationships Couple', 'exclusinary': 'exclusionary', 'himdus': 'hindus',\n",
    "                  'Milo Yianopolous': 'a British polemicist', 'hidusim': 'hinduism',\n",
    "                  'holocaustable': 'holocaust', 'evangilitacal': 'evangelical', 'Busscas': 'Buscas',\n",
    "                  'holocaustal': 'holocaust', 'incestious': 'incestuous', 'Tennesseus': 'Tennessee',\n",
    "                  'GusDur': 'Gus Dur',\n",
    "                  'RPatah - Tan Eng Hwan': 'Silsilah', 'Reinfectus': 'reinfect', 'pharisaistic': 'pharisaism',\n",
    "                  'nuslims': 'Muslims', 'taskus': '', 'musims': 'Muslims',\n",
    "                  'Musevi': 'the independence of Mexico', ' racious ': 'discrimination expression of racism',\n",
    "                  'Muslimophobia': 'Muslim phobia', 'justyfied': 'justified', 'holocause': 'holocaust',\n",
    "                  'musilim': 'Muslim', 'misandrous': 'misandry', 'glrous': 'glorious', 'desemated': 'decimated',\n",
    "                  'votebanks': 'vote banks', 'Parkistan': 'Pakistan', 'Eurooe': 'Europe', 'animlaistic': 'animalistic',\n",
    "                  'Asiasoid': 'Asian', 'Congoid': 'Congolese', 'inheritantly': 'inherently',\n",
    "                  'Asianisation': 'Becoming Asia',\n",
    "                  'Russosphere': 'russia sphere of influence', 'exMuslims': 'Ex-Muslims',\n",
    "                  'discriminatein': 'discrimination', ' hinus ': ' hindus ', 'Nibirus': 'Nibiru',\n",
    "                  'habius - corpus': 'habeas corpus', 'prentious': 'pretentious', 'Sussia': 'ancient Jewish village',\n",
    "                  'moustachess': 'moustaches', 'Russions': 'Russians', 'Yuguslavia': 'Yugoslavia',\n",
    "                  'atrocitties': 'atrocities', 'Muslimophobe': 'Muslim phobic', 'fallicious': 'fallacious',\n",
    "                  'recussed': 'recursed', '@ usafmonitor': '', 'lustfly': 'lustful', 'canMuslims': 'can Muslims',\n",
    "                  'journalust': 'journalist', 'digustingly': 'disgustingly', 'harasing': 'harassing',\n",
    "                  'greatuncle': 'great uncle', 'Drumpf': 'Trump', 'rejectes': 'rejected', 'polyagamous': 'polygamous',\n",
    "                  'Mushlims': 'Muslims', 'accusition': 'accusation', 'geniusses': 'geniuses',\n",
    "                  'moustachesomething': 'moustache something', 'heineous': 'heinous',\n",
    "                  'Sapiosexuals': 'sapiosexual', 'sapiosexuals': 'sapiosexual', 'Sapiosexual': 'sapiosexual',\n",
    "                  'sapiosexual': 'Sexually attracted to intelligence', 'pansexuals': 'pansexual',\n",
    "                  'autosexual': 'auto sexual', 'sexualSlutty': 'sexual Slutty', 'hetorosexuality': 'hetoro sexuality',\n",
    "                  'chinesese': 'chinese', 'pizza gate': 'debunked conspiracy theory',\n",
    "                  'countryless': 'Having no country',\n",
    "                  'muslimare': 'Muslim are', 'iPhoneX': 'iPhone', 'lionese': 'lioness', 'marionettist': 'Marionettes',\n",
    "                  'demonetize': 'demonetized', 'eneyone': 'anyone', 'Karonese': 'Karo people Indonesia',\n",
    "                  'minderheid': 'minder worse', 'mainstreamly': 'mainstream', 'contraproductive': 'contra productive',\n",
    "                  'diffenky': 'differently', 'abandined': 'abandoned', 'p0 rnstars': 'pornstars',\n",
    "                  'overproud': 'over proud',\n",
    "                  'cheekboned': 'cheek boned', 'heriones': 'heroines', 'eventhogh': 'even though',\n",
    "                  'americanmedicalassoc': 'american medical assoc', 'feelwhen': 'feel when', 'Hhhow': 'how',\n",
    "                  'reallySemites': 'really Semites', 'gamergaye': 'gamersgate', 'manspreading': 'man spreading',\n",
    "                  'thammana': 'Tamannaah Bhatia', 'dogmans': 'dogmas', 'managementskills': 'management skills',\n",
    "                  'mangoliod': 'mongoloid', 'geerymandered': 'gerrymandered', 'mandateing': 'man dateing',\n",
    "                  'Romanium': 'Romanum',\n",
    "                  'mailwoman': 'mail woman', 'humancoalition': 'human coalition',\n",
    "                  'manipullate': 'manipulate', 'everyo0 ne': 'everyone', 'takeove': 'takeover',\n",
    "                  'Nonchristians': 'Non Christians', 'goverenments': 'governments', 'govrment': 'government',\n",
    "                  'polygomists': 'polygamists', 'Demogorgan': 'Demogorgon', 'maralago': 'Mar-a-Lago',\n",
    "                  'antibigots': 'anti bigots', 'gouing': 'going', 'muzaffarbad': 'muzaffarabad',\n",
    "                  'suchvstupid': 'such stupid', 'apartheidisrael': 'apartheid israel', \n",
    "                  'personaltiles': 'personal titles', 'lawyergirlfriend': 'lawyer girl friend',\n",
    "                  'northestern': 'northwestern', 'yeardold': 'years old', 'masskiller': 'mass killer',\n",
    "                  'southeners': 'southerners', 'Unitedstatesian': 'United states',\n",
    "\n",
    "                  'peoplekind': 'people kind', 'peoplelike': 'people like', 'countrypeople': 'country people',\n",
    "                  'shitpeople': 'shit people', 'trumpology': 'trump ology', 'trumpites': 'Trump supporters',\n",
    "                  'trumplies': 'trump lies', 'donaldtrumping': 'donald trumping', 'trumpdating': 'trump dating',\n",
    "                  'trumpsters': 'trumpeters', 'ciswomen': 'cis women', 'womenizer': 'womanizer',\n",
    "                  'pregnantwomen': 'pregnant women', 'autoliker': 'auto liker', 'smelllike': 'smell like',\n",
    "                  'autolikers': 'auto likers', 'religiouslike': 'religious like', 'likemail': 'like mail',\n",
    "                  'fislike': 'dislike', 'sneakerlike': 'sneaker like', 'like⬇': 'like',\n",
    "                  'likelovequotes': 'like lovequotes', 'likelogo': 'like logo', 'sexlike': 'sex like',\n",
    "                  'Whatwould': 'What would', 'Howwould': 'How would', 'manwould': 'man would',\n",
    "                  'exservicemen': 'ex servicemen', 'femenism': 'feminism', 'devopment': 'development',\n",
    "                  'doccuments': 'documents', 'supplementplatform': 'supplement platform', 'mendatory': 'mandatory',\n",
    "                  'moviments': 'movements', 'Kremenchuh': 'Kremenchug', 'docuements': 'documents',\n",
    "                  'determenism': 'determinism', 'envisionment': 'envision ment',\n",
    "                  'tricompartmental': 'tri compartmental', 'AddMovement': 'Add Movement',\n",
    "                  'mentionong': 'mentioning', 'Whichtreatment': 'Which treatment', 'repyament': 'repayment',\n",
    "                  'insemenated': 'inseminated', 'inverstment': 'investment',\n",
    "                  'managemental': 'manage mental', 'Inviromental': 'Environmental', 'menstrution': 'menstruation',\n",
    "                  'indtrument': 'instrument', 'mentenance': 'maintenance', 'fermentqtion': 'fermentation',\n",
    "                  'achivenment': 'achievement', 'mismanagements': 'mis managements', 'requriment': 'requirement',\n",
    "                  'denomenator': 'denominator', 'drparment': 'department', 'acumens': 'acumen s',\n",
    "                  'celemente': 'Clemente', 'manajement': 'management', 'govermenent': 'government',\n",
    "                  'accomplishmments': 'accomplishments', 'rendementry': 'rendement ry',\n",
    "                  'repariments': 'departments', 'menstrute': 'menstruate', 'determenistic': 'deterministic',\n",
    "                  'resigment': 'resignment', 'selfpayment': 'self payment', 'imrpovement': 'improvement',\n",
    "                  'enivironment': 'environment', 'compartmentley': 'compartment',\n",
    "                  'augumented': 'augmented', 'parmenent': 'permanent', 'dealignment': 'de alignment',\n",
    "                  'develepoments': 'developments', 'menstrated': 'menstruated', 'phnomenon': 'phenomenon',\n",
    "                  'Employmment': 'Employment', 'dimensionalise': 'dimensional ise', 'menigioma': 'meningioma',\n",
    "                  'recrument': 'recrement', 'Promenient': 'Provenient', 'gonverment': 'government',\n",
    "                  'statemment': 'statement', 'recuirement': 'requirement', 'invetsment': 'investment',\n",
    "                  'parilment': 'parchment', 'parmently': 'patiently', 'agreementindia': 'agreement india',\n",
    "                  'menifesto': 'manifesto', 'accomplsihments': 'accomplishments', 'disangagement': 'disengagement',\n",
    "                  'aevelopment': 'development', 'procument': 'procumbent', 'harashment': 'harassment',\n",
    "                  'Tiannanmen': 'Tiananmen', 'commensalisms': 'commensal isms', 'devlelpment': 'development',\n",
    "                  'dimensons': 'dimensions', 'recruitment2017': 'recruitment 2017', 'polishment': 'pol ishment',\n",
    "                  'CommentSafe': 'Comment Safe', 'meausrements': 'measurements', 'geomentrical': 'geometrical',\n",
    "                  'undervelopment': 'undevelopment', 'mensurational': 'mensuration al', 'fanmenow': 'fan menow',\n",
    "                  'permenganate': 'permanganate', 'bussinessmen': 'businessmen',\n",
    "                  'supertournaments': 'super tournaments', 'permanmently': 'permanently',\n",
    "                  'lamenectomy': 'lamnectomy', 'assignmentcanyon': 'assignment canyon', 'adgestment': 'adjustment',\n",
    "                  'mentalized': 'metalized', 'docyments': 'documents', 'requairment': 'requirement',\n",
    "                  'batsmencould': 'batsmen could', 'argumentetc': 'argument etc', 'enjoiment': 'enjoyment',\n",
    "                  'invement': 'movement', 'accompliushments': 'accomplishments', 'regements': 'regiments',\n",
    "                  'departmentHow': 'department How', 'Aremenian': 'Armenian', 'amenclinics': 'amen clinics',\n",
    "                  'nonfermented': 'non fermented', 'Instumentation': 'Instrumentation', 'mentalitiy': 'mentality',\n",
    "                  ' govermen ': 'goverment', 'underdevelopement': 'under developement', 'parlimentry': 'parliamentary',\n",
    "                  'indemenity': 'indemnity', 'Inatrumentation': 'Instrumentation', 'menedatory': 'mandatory',\n",
    "                  'mentiri': 'entire', 'accomploshments': 'accomplishments', 'instrumention': 'instrument ion',\n",
    "                  'afvertisements': 'advertisements', 'parlementarian': 'parlement arian',\n",
    "                  'entitlments': 'entitlements', 'endrosment': 'endorsement', 'improment': 'impriment',\n",
    "                  'archaemenid': 'Achaemenid', 'replecement': 'replacement', 'placdment': 'placement',\n",
    "                  'femenise': 'feminise', 'envinment': 'environment', 'AmenityCompany': 'Amenity Company',\n",
    "                  'increaments': 'increments', 'accomplihsments': 'accomplishments',\n",
    "                  'manygovernment': 'many government', 'panishments': 'punishments', 'elinment': 'eloinment',\n",
    "                  'mendalin': 'mend alin', 'farmention': 'farm ention', 'preincrement': 'pre increment',\n",
    "                  'postincrement': 'post increment', 'achviements': 'achievements', 'menditory': 'mandatory',\n",
    "                  'Emouluments': 'Emoluments', 'Stonemen': 'Stone men', 'menmium': 'medium',\n",
    "                  'entaglement': 'entanglement', 'integumen': 'integument', 'harassument': 'harassment',\n",
    "                  'retairment': 'retainment', 'enviorement': 'environment', 'tormentous': 'torment ous',\n",
    "                  'confiment': 'confident', 'Enchroachment': 'Encroachment', 'prelimenary': 'preliminary',\n",
    "                  'fudamental': 'fundamental', 'instrumenot': 'instrument', 'icrement': 'increment',\n",
    "                  'prodimently': 'prominently', 'meniss': 'menise', 'Whoimplemented': 'Who implemented',\n",
    "                  'Representment': 'Rep resentment', 'StartFragment': 'Start Fragment',\n",
    "                  'EndFragment': 'End Fragment', ' documentarie ': ' documentaries ', 'requriments': 'requirements',\n",
    "                  'constitutionaldevelopment': 'constitutional development', 'parlamentarians': 'parliamentarians',\n",
    "                  'Rumenova': 'Rumen ova', 'argruments': 'arguments', 'findamental': 'fundamental',\n",
    "                  'totalinvestment': 'total investment', 'gevernment': 'government', 'recmommend': 'recommend',\n",
    "                  'appsmoment': 'apps moment', 'menstruual': 'menstrual', 'immplemented': 'implemented',\n",
    "                  'engangement': 'engagement', 'invovement': 'involvement', 'returement': 'retirement',\n",
    "                  'simentaneously': 'simultaneously', 'accompishments': 'accomplishments',\n",
    "                  'menstraution': 'menstruation', 'experimently': 'experiment', 'abdimen': 'abdomen',\n",
    "                  'cemenet': 'cement', 'propelment': 'propel ment', 'unamendable': 'un amendable',\n",
    "                  'employmentnews': 'employment news', 'lawforcement': 'law forcement',\n",
    "                  'menstuating': 'menstruating', 'fevelopment': 'development', 'reglamented': 'reg lamented',\n",
    "                  'imrovment': 'improvement', 'recommening': 'recommending', 'sppliment': 'supplement',\n",
    "                  'measument': 'measurement', 'reimbrusement': 'reimbursement', 'Nutrament': 'Nutriment',\n",
    "                  'puniahment': 'punishment', 'subligamentous': 'sub ligamentous', 'comlementry': 'complementary',\n",
    "                  'reteirement': 'retirement', 'envioronments': 'environments', 'haraasment': 'harassment',\n",
    "                  'USAgovernment': 'USA government', 'Apartmentfinder': 'Apartment finder',\n",
    "                  'encironment': 'environment', 'metacompartment': 'meta compartment',\n",
    "                  'augumentation': 'argumentation', 'dsymenorrhoea': 'dysmenorrhoea',\n",
    "                  'nonabandonment': 'non abandonment', 'annoincement': 'announcement',\n",
    "                  'menberships': 'memberships', 'Gamenights': 'Game nights', 'enliightenment': 'enlightenment',\n",
    "                  'supplymentry': 'supplementary', 'parlamentary': 'parliamentary', 'duramen': 'dura men',\n",
    "                  'hotelmanagement': 'hotel management', 'deartment': 'department',\n",
    "                  'treatmentshelp': 'treatments help', 'attirements': 'attire ments',\n",
    "                  'amendmending': 'amend mending', 'pseudomeningocele': 'pseudo meningocele',\n",
    "                  'intrasegmental': 'intra segmental', 'treatmenent': 'treatment', 'infridgement': 'infringement',\n",
    "                  'infringiment': 'infringement', 'recrecommend': 'rec recommend', 'entartaiment': 'entertainment',\n",
    "                  'inplementing': 'implementing', 'indemendent': 'independent', 'tremendeous': 'tremendous',\n",
    "                  'commencial': 'commercial', 'scomplishments': 'accomplishments', 'Emplement': 'Implement',\n",
    "                  'dimensiondimensions': 'dimension dimensions', 'depolyment': 'deployment',\n",
    "                  'conpartment': 'compartment', 'govnments': 'movements', 'menstrat': 'menstruate',\n",
    "                  'accompplishments': 'accomplishments', 'Enchacement': 'Enchancement',\n",
    "                  'developmenent': 'development', 'emmenagogues': 'emmenagogue', 'aggeement': 'agreement',\n",
    "                  'elementsbond': 'elements bond', 'remenant': 'remnant', 'Manamement': 'Management',\n",
    "                  'Augumented': 'Augmented', 'dimensonless': 'dimensionless',\n",
    "                  'ointmentsointments': 'ointments ointments', 'achiements': 'achievements',\n",
    "                  'recurtment': 'recurrent', 'gouverments': 'governments', 'docoment': 'document',\n",
    "                  'programmingassignments': 'programming assignments', 'menifest': 'manifest',\n",
    "                  'investmentguru': 'investment guru', 'deployements': 'deployments', 'Invetsment': 'Investment',\n",
    "                  'plaement': 'placement', 'Perliament': 'Parliament', 'femenists': 'feminists',\n",
    "                  'ecumencial': 'ecumenical', 'advamcements': 'advancements', 'refundment': 'refund ment',\n",
    "                  'settlementtake': 'settlement take', 'mensrooms': 'mens rooms',\n",
    "                  'productManagement': 'product Management', 'armenains': 'armenians',\n",
    "                  'betweenmanagement': 'between management', 'difigurement': 'disfigurement',\n",
    "                  'Armenized': 'Armenize', 'hurrasement': 'hurra sement', 'mamgement': 'management',\n",
    "                  'momuments': 'monuments', 'eauipments': 'equipments', 'managemenet': 'management',\n",
    "                  'treetment': 'treatment', 'webdevelopement': 'web developement', 'supplemenary': 'supplementary',\n",
    "                  'Encironmental': 'Environmental', 'Understandment': 'Understand ment',\n",
    "                  'enrollnment': 'enrollment', 'thinkstrategic': 'think strategic', 'thinkinh': 'thinking',\n",
    "                  'Softthinks': 'Soft thinks', 'underthinking': 'under thinking', 'thinksurvey': 'think survey',\n",
    "                  'whitelash': 'white lash', 'whiteheds': 'whiteheads', 'whitetning': 'whitening',\n",
    "                  'whitegirls': 'white girls', 'whitewalkers': 'white walkers', 'manycountries': 'many countries',\n",
    "                  'accomany': 'accompany', 'fromGermany': 'from Germany', 'manychat': 'many chat',\n",
    "                  'Germanyl': 'Germany l', 'manyness': 'many ness', 'many4': 'many', 'exmuslims': 'ex muslims',\n",
    "                  'digitizeindia': 'digitize india', 'indiarush': 'india rush', 'indiareads': 'india reads',\n",
    "                  'telegraphindia': 'telegraph india', 'Southindia': 'South india', 'Airindia': 'Air india',\n",
    "                  'siliconindia': 'silicon india', 'airindia': 'air india', 'indianleaders': 'indian leaders',\n",
    "                  'fundsindia': 'funds india', 'indianarmy': 'indian army', 'Technoindia': 'Techno india',\n",
    "                  'Betterindia': 'Better india', 'capesindia': 'capes india', 'Rigetti': 'Ligetti',\n",
    "                  'vegetablr': 'vegetable', 'get90': 'get', 'Magetta': 'Maretta', 'nagetive': 'native',\n",
    "                  'isUnforgettable': 'is Unforgettable', 'get630': 'get 630', 'GadgetPack': 'Gadget Pack',\n",
    "                  'Languagetool': 'Language tool', 'bugdget': 'budget', 'africaget': 'africa get',\n",
    "                  'ABnegetive': 'Abnegative', 'orangetheory': 'orange theory', 'getsmuggled': 'get smuggled',\n",
    "                  'avegeta': 'ave geta', 'gettubg': 'getting', 'gadgetsnow': 'gadgets now',\n",
    "                  'surgetank': 'surge tank', 'gadagets': 'gadgets', 'getallparts': 'get allparts',\n",
    "                  'messenget': 'messenger', 'vegetarean': 'vegetarian', 'get1000': 'get 1000',\n",
    "                  'getfinancing': 'get financing', 'getdrip': 'get drip', 'AdsTargets': 'Ads Targets',\n",
    "                  'tgethr': 'together', 'vegetaries': 'vegetables', 'forgetfulnes': 'forgetfulness',\n",
    "                  'fisgeting': 'fidgeting', 'BudgetAir': 'Budget Air',\n",
    "                  'getDepersonalization': 'get Depersonalization', 'negetively': 'negatively',\n",
    "                  'gettibg': 'getting', 'nauget': 'naught', 'Bugetti': 'Bugatti', 'plagetum': 'plage tum',\n",
    "                  'vegetabale': 'vegetable', 'changetip': 'change tip', 'blackwashing': 'black washing',\n",
    "                  'blackpink': 'black pink', 'blackmoney': 'black money',\n",
    "                  'blackmarks': 'black marks', 'blackbeauty': 'black beauty', 'unblacklisted': 'un blacklisted',\n",
    "                  'blackdotes': 'black dotes', 'blackboxing': 'black boxing', 'blackpaper': 'black paper',\n",
    "                  'blackpower': 'black power', 'Latinamericans': 'Latin americans', 'musigma': 'mu sigma',\n",
    "                  'Indominus': 'In dominus', 'usict': 'USSCt', 'indominus': 'in dominus', 'Musigma': 'Mu sigma',\n",
    "                  'plus5': 'plus', 'Russiagate': 'Russia gate', 'russophobic': 'Russophobiac',\n",
    "                  'Marcusean': 'Marcuse an', 'Radijus': 'Radius', 'cobustion': 'combustion',\n",
    "                  'Austrialians': 'Australians', 'mylogenous': 'myogenous', 'Raddus': 'Radius',\n",
    "                  'hetrogenous': 'heterogenous', 'greenhouseeffect': 'greenhouse effect', 'aquous': 'aqueous',\n",
    "                  'Taharrush': 'Tahar rush', 'Senousa': 'Venous', 'diplococcus': 'diplo coccus',\n",
    "                  'CityAirbus': 'City Airbus', 'sponteneously': 'spontaneously', 'trustless': 't rustless',\n",
    "                  'Pushkaram': 'Pushkara m', 'Fusanosuke': 'Fu sanosuke', 'isthmuses': 'isthmus es',\n",
    "                  'lucideus': 'lucidum', 'overjustification': 'over justification', 'Bindusar': 'Bind usar',\n",
    "                  'cousera': 'couler', 'musturbation': 'masturbation', 'infustry': 'industry',\n",
    "                  'Huswifery': 'Huswife ry', 'rombous': 'bombous', 'disengenuously': 'disingenuously',\n",
    "                  'sllybus': 'syllabus', 'celcious': 'delicious', 'cellsius': 'celsius',\n",
    "                  'lethocerus': 'Lethocerus', 'monogmous': 'monogamous', 'Ballyrumpus': 'Bally rumpus',\n",
    "                  'Koushika': 'Koushik a', 'vivipoarous': 'viviparous', 'ludiculous': 'ridiculous',\n",
    "                  'sychronous': 'synchronous', 'industiry': 'industry', 'scuduse': 'scud use',\n",
    "                  'babymust': 'baby must', 'simultqneously': 'simultaneously', 'exust': 'ex ust',\n",
    "                  'notmusing': 'not musing', 'Zamusu': 'Amuse', 'tusaki': 'tu saki', 'Marrakush': 'Marrakesh',\n",
    "                  'justcheaptickets': 'just cheaptickets', 'Ayahusca': 'Ayahausca', 'samousa': 'samosa',\n",
    "                  'Gusenberg': 'Gutenberg', 'illustratuons': 'illustrations', 'extemporeneous': 'extemporaneous',\n",
    "                  'Mathusla': 'Mathusala', 'Confundus': 'Con fundus', 'tusts': 'trusts', 'poisenious': 'poisonous',\n",
    "                  'Mevius': 'Medius', 'inuslating': 'insulating', 'aroused21000': 'aroused 21000',\n",
    "                  'Wenzeslaus': 'Wenceslaus', 'JustinKase': 'Justin Kase', 'purushottampur': 'purushottam pur',\n",
    "                  'citruspay': 'citrus pay', 'secutus': 'sects', 'austentic': 'austenitic',\n",
    "                  'FacePlusPlus': 'Face PlusPlus', 'aysnchronous': 'asynchronous',\n",
    "                  'teamtreehouse': 'team treehouse', 'uncouncious': 'unconscious', 'Priebuss': 'Prie buss',\n",
    "                  'consciousuness': 'consciousness', 'susubsoil': 'su subsoil', 'trimegistus': 'Trismegistus',\n",
    "                  'protopeterous': 'protopterous', 'trustworhty': 'trustworthy', 'ushually': 'usually',\n",
    "                  'industris': 'industries', 'instantneous': 'instantaneous', 'superplus': 'super plus',\n",
    "                  'shrusti': 'shruti', 'hindhus': 'hindus', 'outonomous': 'autonomous', 'reliegious': 'religious',\n",
    "                  'Kousakis': 'Kou sakis', 'reusult': 'result', 'JanusGraph': 'Janus Graph',\n",
    "                  'palusami': 'palus ami', 'mussraff': 'muss raff', 'hukous': 'humous',\n",
    "                  'photoacoustics': 'photo acoustics', 'kushanas': 'kusha nas', 'justdile': 'justice',\n",
    "                  'Massahusetts': 'Massachusetts', 'uspset': 'upset', 'sustinet': 'sustinent',\n",
    "                  'consicious': 'conscious', 'Sadhgurus': 'Sadh gurus', 'hystericus': 'hysteric us',\n",
    "                  'visahouse': 'visa house', 'supersynchronous': 'super synchronous', 'posinous': 'rosinous',\n",
    "                  'Fernbus': 'Fern bus', 'Tiltbrush': 'Tilt brush', 'glueteus': 'gluteus', 'posionus': 'poisons',\n",
    "                  'Freus': 'Frees', 'Zhuchengtyrannus': 'Zhucheng tyrannus', 'savonious': 'sanious',\n",
    "                  'CusJo': 'Cusco', 'congusion': 'confusion', 'dejavus': 'dejavu s', 'uncosious': 'uncopious',\n",
    "                  'previius': 'previous', 'counciousness': 'conciousness', 'lustorus': 'lustrous',\n",
    "                  'sllyabus': 'syllabus', 'mousquitoes': 'mosquitoes', 'Savvius': 'Savvies', 'arceius': 'Arcesius',\n",
    "                  'prejusticed': 'prejudiced', 'requsitioned': 'requisitioned',\n",
    "                  'deindustralization': 'deindustrialization', 'muscleblaze': 'muscle blaze',\n",
    "                  'ConsciousX5': 'conscious', 'nitrogenious': 'nitrogenous', 'mauritious': 'mauritius',\n",
    "                  'rigrously': 'rigorously', 'Yutyrannus': 'Yu tyrannus', 'muscualr': 'muscular',\n",
    "                  'conscoiusness': 'consciousness', 'Causians': 'Crusians', 'WorkFusion': 'Work Fusion',\n",
    "                  'puspak': 'pu spak', 'Inspirus': 'Inspires', 'illiustrations': 'illustrations',\n",
    "                  'Nobushi': 'No bushi', 'theuseof': 'thereof', 'suspicius': 'suspicious', 'Intuous': 'Virtuous',\n",
    "                  'gaushalas': 'gaus halas', 'campusthrough': 'campus through', 'seriousity': 'seriosity',\n",
    "                  'resustence': 'resistence', 'geminatus': 'geminates', 'disquss': 'discuss',\n",
    "                  'nicholus': 'nicholas', 'Husnai': 'Hussar', 'diiscuss': 'discuss', 'diffussion': 'diffusion',\n",
    "                  'phusicist': 'physicist', 'ernomous': 'enormous', 'Khushali': 'Khushal i', 'heitus': 'Leitus',\n",
    "                  'cracksbecause': 'cracks because', 'Nautlius': 'Nautilus', 'trausted': 'trusted',\n",
    "                  'Dardandus': 'Dardanus', 'Megatapirus': 'Mega tapirus', 'clusture': 'culture',\n",
    "                  'vairamuthus': 'vairamuthu s', 'disclousre': 'disclosure',\n",
    "                  'industrilaization': 'industrialization', 'musilms': 'muslims', 'Australia9': 'Australian',\n",
    "                  'causinng': 'causing', 'ibdustries': 'industries', 'searious': 'serious',\n",
    "                  'Coolmuster': 'Cool muster', 'sissyphus': 'sisyphus', ' justificatio ': 'justification',\n",
    "                  'antihindus': 'anti hindus', 'Moduslink': 'Modus link', 'zymogenous': 'zymogen ous',\n",
    "                  'prospeorus': 'prosperous', 'Retrocausality': 'Retro causality', 'FusionGPS': 'Fusion GPS',\n",
    "                  'Mouseflow': 'Mouse flow', 'bootyplus': 'booty plus', 'Itylus': 'I tylus',\n",
    "                  'Olnhausen': 'Olshausen', 'suspeect': 'suspect', 'entusiasta': 'enthusiast',\n",
    "                  'fecetious': 'facetious', 'bussiest': 'fussiest', 'Draconius': 'Draconis',\n",
    "                  'requsite': 'requisite', 'nauseatic': 'nausea tic', 'Brusssels': 'Brussels',\n",
    "                  'repurcussion': 'repercussion', 'Jeisus': 'Jesus', 'philanderous': 'philander ous',\n",
    "                  'muslisms': 'muslims', 'august2017': 'august 2017', 'calccalculus': 'calc calculus',\n",
    "                  'unanonymously': 'un anonymously', 'Imaprtus': 'Impetus', 'carnivorus': 'carnivorous',\n",
    "                  'Corypheus': 'Coryphees', 'austronauts': 'astronauts', 'neucleus': 'nucleus',\n",
    "                  'housepoor': 'house poor', 'rescouses': 'responses', 'Tagushi': 'Tagus hi',\n",
    "                  'hyperfocusing': 'hyper focusing', 'nutriteous': 'nutritious', 'chylus': 'chylous',\n",
    "                  'preussure': 'pressure', 'outfocus': 'out focus', 'Hanfus': 'Hannus', 'Rustyrose': 'Rusty rose',\n",
    "                  'vibhushant': 'vibhushan t', 'conciousnes': 'conciousness', 'Venus25': 'Venus',\n",
    "                  'Sedataious': 'Seditious', 'promuslim': 'pro muslim', 'statusGuru': 'status Guru',\n",
    "                  'yousician': 'musician', 'transgenus': 'trans genus', 'Pushbullet': 'Push bullet',\n",
    "                  'jeesyllabus': 'jee syllabus', 'complusary': 'compulsory', 'Holocoust': 'Holocaust',\n",
    "                  'careerplus': 'career plus', 'Lllustrate': 'Illustrate', 'Musino': 'Musion',\n",
    "                  'Phinneus': 'Phineus', 'usedtoo': 'used too', 'JustBasic': 'Just Basic', 'webmusic': 'web music',\n",
    "                  'TrustKit': 'Trust Kit', 'industrZgies': 'industries', 'rubustness': 'robustness',\n",
    "                  'Missuses': 'Miss uses', 'Musturbation': 'Masturbation', 'bustees': 'bus tees',\n",
    "                  'justyfy': 'justify', 'pegusus': 'pegasus', 'industrybuying': 'industry buying',\n",
    "                  'advantegeous': 'advantageous', 'kotatsus': 'kotatsu s', 'justcreated': 'just created',\n",
    "                  'simultameously': 'simultaneously', 'husoone': 'huso one', 'twiceusing': 'twice using',\n",
    "                  'cetusplay': 'cetus play', 'sqamous': 'squamous', 'claustophobic': 'claustrophobic',\n",
    "                  'Kaushika': 'Kaushik a', 'dioestrus': 'di oestrus', 'Degenerous': 'De generous',\n",
    "                  'neculeus': 'nucleus', 'cutaneously': 'cu taneously', 'Alamotyrannus': 'Alamo tyrannus',\n",
    "                  'Ivanious': 'Avanious', 'arceous': 'araceous', 'Flixbus': 'Flix bus', 'caausing': 'causing',\n",
    "                  'publious': 'Publius', 'Juilus': 'Julius', 'Australianism': 'Australian ism',\n",
    "                  'vetronus': 'verrons', 'nonspontaneous': 'non spontaneous', 'calcalus': 'calculus',\n",
    "                  'commudus': 'Commodus', 'Rheusus': 'Rhesus', 'syallubus': 'syllabus', 'Yousician': 'Musician',\n",
    "                  'qurush': 'qu rush', 'athiust': 'athirst', 'conclusionless': 'conclusion less',\n",
    "                  'usertesting': 'user testing', 'redius': 'radius', 'Austrolia': 'Australia',\n",
    "                  'sllaybus': 'syllabus', 'toponymous': 'top onymous', 'businiss': 'business',\n",
    "                  'hyperthalamus': 'hyper thalamus', 'clause55': 'clause', 'cosicous': 'conscious',\n",
    "                  'Sushena': 'Saphena', 'Luscinus': 'Luscious', 'Prussophile': 'Russophile', 'jeaslous': 'jealous',\n",
    "                  'Austrelia': 'Australia', 'contiguious': 'contiguous',\n",
    "                  'subconsciousnesses': 'sub consciousnesses', ' jusification ': 'justification',\n",
    "                  'dilusion': 'delusion', 'anticoncussive': 'anti concussive', 'disngush': 'disgust',\n",
    "                  'constiously': 'consciously', 'filabustering': 'filibustering', 'GAPbuster': 'GAP buster',\n",
    "                  'insectivourous': 'insectivorous', 'glocuse': 'louse', 'Antritrust': 'Antitrust',\n",
    "                  'thisAustralian': 'this Australian', 'FusionDrive': 'Fusion Drive', 'nuclus': 'nucleus',\n",
    "                  'abussive': 'abusive', 'mustang1': 'mustangs', 'inradius': 'in radius', 'polonious': 'polonius',\n",
    "                  'ofKulbhushan': 'of Kulbhushan', 'homosporous': 'homos porous', 'circumradius': 'circum radius',\n",
    "                  'atlous': 'atrous', 'insustry': 'industry', 'campuswith': 'campus with', 'beacsuse': 'because',\n",
    "                  'concuous': 'conscious', 'nonHindus': 'non Hindus', 'carnivourous': 'carnivorous',\n",
    "                  'tradeplus': 'trade plus', 'Jeruselam': 'Jerusalem',\n",
    "                  'musuclar': 'muscular', 'deangerous': 'dangerous', 'disscused': 'discussed',\n",
    "                  'industdial': 'industrial', 'sallatious': 'fallacious', 'rohmbus': 'rhombus',\n",
    "                  'golusu': 'gol usu', 'Minangkabaus': 'Minangkabau s', 'Mustansiriyah': 'Mustansiriya h',\n",
    "                  'anomymously': 'anonymously', 'abonymously': 'anonymously', 'indrustry': 'industry',\n",
    "                  'Musharrf': 'Musharraf', 'workouses': 'workhouses', 'sponataneously': 'spontaneously',\n",
    "                  'anmuslim': 'an muslim', 'syallbus': 'syllabus', 'presumptuousnes': 'presumptuousness',\n",
    "                  'Thaedus': 'Thaddus', 'industey': 'industry', 'hkust': 'hust', 'Kousseri': 'Kousser i',\n",
    "                  'mousestats': 'mouses tats', 'russiagate': 'russia gate', 'simantaneously': 'simultaneously',\n",
    "                  'Austertana': 'Auster tana', 'infussions': 'infusions', 'coclusion': 'conclusion',\n",
    "                  'sustainabke': 'sustainable', 'tusami': 'tu sami', 'anonimously': 'anonymously',\n",
    "                  'usebase': 'use base', 'balanoglossus': 'Balanoglossus', 'Unglaus': 'Ung laus',\n",
    "                  'ignoramouses': 'ignoramuses', 'snuus': 'snugs', 'reusibility': 'reusability',\n",
    "                  'Straussianism': 'Straussian ism', 'simoultaneously': 'simultaneously',\n",
    "                  'realbonus': 'real bonus', 'nuchakus': 'nunchakus', 'annonimous': 'anonymous',\n",
    "                  'Incestious': 'Incestuous', 'Manuscriptology': 'Manuscript ology', 'difusse': 'diffuse',\n",
    "                  'Pliosaurus': 'Pliosaur us', 'cushelle': 'cush elle', 'Catallus': 'Catullus',\n",
    "                  'MuscleBlaze': 'Muscle Blaze', 'confousing': 'confusing', 'enthusiasmless': 'enthusiasm less',\n",
    "                  'Tetherusd': 'Tethered', 'Josephius': 'Josephus', 'jusrlt': 'just',\n",
    "                  'simutaneusly': 'simultaneously', 'mountaneous': 'mountainous', 'Badonicus': 'Sardonicus',\n",
    "                  'muccus': 'mucous', 'nicus': 'nidus', 'austinlizards': 'austin lizards',\n",
    "                  'errounously': 'erroneously', 'Australua': 'Australia', 'sylaabus': 'syllabus',\n",
    "                  'dusyant': 'distant', 'javadiscussion': 'java discussion', 'megabuses': 'mega buses',\n",
    "                  'danergous': 'dangerous', 'contestious': 'contentious', 'exause': 'excuse',\n",
    "                  'muscluar': 'muscular', 'avacous': 'vacuous', 'Ingenhousz': 'Ingenious',\n",
    "                  'holocausting': 'holocaust ing', 'Pakustan': 'Pakistan', 'purusharthas': 'purushartha',\n",
    "                  'bapus': 'bapu s', 'useul': 'useful', 'pretenious': 'pretentious', 'homogeneus': 'homogeneous',\n",
    "                  'bhlushes': 'blushes', 'Saggittarius': 'Sagittarius', 'sportsusa': 'sports usa',\n",
    "                  'kerataconus': 'keratoconus', 'infrctuous': 'infectuous', 'Anonoymous': 'Anonymous',\n",
    "                  'triphosphorus': 'tri phosphorus', 'ridicjlously': 'ridiculously',\n",
    "                  'worldbusiness': 'world business', 'hollcaust': 'holocaust', 'Dusra': 'Dura',\n",
    "                  'meritious': 'meritorious', 'Sauskes': 'Causes', 'inudustry': 'industry',\n",
    "                  'frustratd': 'frustrate', 'hypotenous': 'hypogenous', 'Dushasana': 'Dush asana',\n",
    "                  'saadus': 'status', 'keratokonus': 'keratoconus', 'Jarrus': 'Harrus', 'neuseous': 'nauseous',\n",
    "                  'simutanously': 'simultaneously', 'diphosphorus': 'di phosphorus', 'sulprus': 'surplus',\n",
    "                  'Hasidus': 'Hasid us', 'suspenive': 'suspensive', 'illlustrator': 'illustrator',\n",
    "                  'userflows': 'user flows', 'intrusivethoughts': 'intrusive thoughts', 'countinous': 'continuous',\n",
    "                  'gpusli': 'gusli', 'Calculus1': 'Calculus', 'bushiri': 'Bushire',\n",
    "                  'torvosaurus': 'Torosaurus', 'chestbusters': 'chest busters', 'Satannus': 'Sat annus',\n",
    "                  'falaxious': 'fallacious', 'obnxious': 'obnoxious', 'tranfusions': 'transfusions',\n",
    "                  'PlayMagnus': 'Play Magnus', 'Epicodus': 'Episodes', 'Hypercubus': 'Hypercubes',\n",
    "                  'Musickers': 'Musick ers', 'programmebecause': 'programme because', 'indiginious': 'indigenous',\n",
    "                  'housban': 'Housman', 'iusso': 'kusso', 'annilingus': 'anilingus', 'Nennus': 'Genius',\n",
    "                  'pussboy': 'puss boy', 'Photoacoustics': 'Photo acoustics', 'Hindusthanis': 'Hindustanis',\n",
    "                  'lndustrial': 'industrial', 'tyrannously': 'tyrannous', 'Susanoomon': 'Susanoo mon',\n",
    "                  'colmbus': 'columbus', 'sussessful': 'successful', 'ousmania': 'ous mania',\n",
    "                  'ilustrating': 'illustrating', 'famousbirthdays': 'famous birthdays',\n",
    "                  'suspectance': 'suspect ance', 'extroneous': 'extraneous', 'teethbrush': 'teeth brush',\n",
    "                  'abcmouse': 'abc mouse', 'degenerous': 'de generous', 'doesGauss': 'does Gauss',\n",
    "                  'insipudus': 'insipidus', 'movielush': 'movie lush', 'Rustichello': 'Rustic hello',\n",
    "                  'Firdausiya': 'Firdausi ya', 'checkusers': 'check users', 'householdware': 'household ware',\n",
    "                  'prosporously': 'prosperously', 'SteLouse': 'Ste Louse', 'obfuscaton': 'obfuscation',\n",
    "                  'amorphus': 'amorph us', 'trustworhy': 'trustworthy', 'celsious': 'cesious',\n",
    "                  'dangorous': 'dangerous', 'anticancerous': 'anti cancerous', 'cousi ': 'cousin ',\n",
    "                  'austroloid': 'australoid', 'fergussion': 'percussion', 'andKyokushin': 'and Kyokushin',\n",
    "                  'cousan': 'cousin', 'Huskystar': 'Hu skystar', 'retrovisus': 'retrovirus', 'becausr': 'because',\n",
    "                  'Jerusalsem': 'Jerusalem', 'motorious': 'notorious', 'industrilised': 'industrialised',\n",
    "                  'powerballsusa': 'powerballs usa', 'monoceious': 'monoecious', 'batteriesplus': 'batteries plus',\n",
    "                  'nonviscuous': 'nonviscous', 'industion': 'induction', 'bussinss': 'bussings',\n",
    "                  'userbags': 'user bags', 'Jlius': 'Julius', 'thausand': 'thousand', 'plustwo': 'plus two',\n",
    "                  'defpush': 'def push', 'subconcussive': 'sub concussive', 'muslium': 'muslim',\n",
    "                  'industrilization': 'industrialization', 'Maurititus': 'Mauritius', 'uslme': 'some',\n",
    "                  'Susgaon': 'Surgeon', 'Pantherous': 'Panther ous', 'antivirius': 'antivirus',\n",
    "                  'Trustclix': 'Trust clix', 'silumtaneously': 'simultaneously', 'Icompus': 'Corpus',\n",
    "                  'atonomous': 'autonomous', 'Reveuse': 'Reve use', 'legumnous': 'leguminous',\n",
    "                  'syllaybus': 'syllabus', 'louspeaker': 'loudspeaker', 'susbtraction': 'substraction',\n",
    "                  'virituous': 'virtuous', 'disastrius': 'disastrous', 'jerussalem': 'jerusalem',\n",
    "                  'Industrailzed': 'Industrialized', 'recusion': 'recushion',\n",
    "                  'simultenously': 'simultaneously',\n",
    "                  'Pulphus': 'Pulpous', 'harbaceous': 'herbaceous', 'phlegmonous': 'phlegmon ous', 'use38': 'use',\n",
    "                  'jusify': 'justify', 'instatanously': 'instantaneously', 'tetramerous': 'tetramer ous',\n",
    "                  'usedvin': 'used vin', 'sagittarious': 'sagittarius', 'mausturbate': 'masturbate',\n",
    "                  'subcautaneous': 'subcutaneous', 'dangergrous': 'dangerous', 'sylabbus': 'syllabus',\n",
    "                  'hetorozygous': 'heterozygous', 'Ignasius': 'Ignacius', 'businessbor': 'business bor',\n",
    "                  'Bhushi': 'Thushi', 'Moussolini': 'Mussolini', 'usucaption': 'usu caption',\n",
    "                  'Customzation': 'Customization', 'cretinously': 'cretinous', 'genuiuses': 'geniuses',\n",
    "                  'Moushmee': 'Mousmee', 'neigous': 'nervous',\n",
    "                  'infrustructre': 'infrastructure', 'Ilusha': 'Ilesha', 'suconciously': 'unconciously',\n",
    "                  'stusy': 'study', 'mustectomy': 'mastectomy', 'Farmhousebistro': 'Farmhouse bistro',\n",
    "                  'instantanous': 'instantaneous', 'JustForex': 'Just Forex', 'Indusyry': 'Industry',\n",
    "                  'mustabating': 'must abating', 'uninstrusive': 'unintrusive', 'customshoes': 'customs hoes',\n",
    "                  'homageneous': 'homogeneous', 'Empericus': 'Imperious', 'demisexuality': 'demi sexuality',\n",
    "                  'transexualism': 'transsexualism', 'sexualises': 'sexualise', 'demisexuals': 'demisexual',\n",
    "                  'sexuly': 'sexily', 'Pornosexuality': 'Porno sexuality', 'sexond': 'second', 'sexxual': 'sexual',\n",
    "                  'asexaul': 'asexual', 'sextactic': 'sex tactic', 'sexualityism': 'sexuality ism',\n",
    "                  'monosexuality': 'mono sexuality', 'intwrsex': 'intersex', 'hypersexualize': 'hyper sexualize',\n",
    "                  'homosexualtiy': 'homosexuality', 'examsexams': 'exams exams', 'sexmates': 'sex mates',\n",
    "                  'sexyjobs': 'sexy jobs', 'sexitest': 'sexiest', 'fraysexual': 'fray sexual',\n",
    "                  'sexsurrogates': 'sex surrogates', 'sexuallly': 'sexually', 'gamersexual': 'gamer sexual',\n",
    "                  'greysexual': 'grey sexual', 'omnisexuality': 'omni sexuality', 'hetereosexual': 'heterosexual',\n",
    "                  'productsexamples': 'products examples', 'sexgods': 'sex gods', 'semisexual': 'semi sexual',\n",
    "                  'homosexulity': 'homosexuality', 'sexeverytime': 'sex everytime', 'neurosexist': 'neuro sexist',\n",
    "                  'worldquant': 'world quant', 'Freshersworld': 'Freshers world', 'smartworld': 'sm artworld',\n",
    "                  'Mistworlds': 'Mist worlds', 'boothworld': 'booth world', 'ecoworld': 'eco world',\n",
    "                  'Ecoworld': 'Eco world', 'underworldly': 'under worldly', 'worldrank': 'world rank',\n",
    "                  'Clearworld': 'Clear world', 'Boothworld': 'Booth world', 'Rimworld': 'Rim world',\n",
    "                  'cryptoworld': 'crypto world', 'machineworld': 'machine world', 'worldwideley': 'worldwide ley',\n",
    "                  'capuletwant': 'capulet want', 'Bhagwanti': 'Bhagwant i', 'Unwanted72': 'Unwanted 72',\n",
    "                  'wantrank': 'want rank',\n",
    "                  'willhappen': 'will happen', 'thateasily': 'that easily',\n",
    "                  'Whatevidence': 'What evidence', 'metaphosphates': 'meta phosphates',\n",
    "                  'exilarchate': 'exilarch ate', 'aulphate': 'sulphate', 'Whateducation': 'What education',\n",
    "                  'persulphates': 'per sulphates', 'disulphate': 'di sulphate', 'picosulphate': 'pico sulphate',\n",
    "                  'tetraosulphate': 'tetrao sulphate', 'prechinese': 'pre chinese',\n",
    "                  'Hellochinese': 'Hello chinese', 'muchdeveloped': 'much developed', 'stomuch': 'stomach',\n",
    "                  'Whatmakes': 'What makes', 'Lensmaker': 'Lens maker', 'eyemake': 'eye make',\n",
    "                  'Techmakers': 'Tech makers', 'cakemaker': 'cake maker', 'makeup411': 'makeup 411',\n",
    "                  'objectmake': 'object make', 'crazymaker': 'crazy maker', 'techmakers': 'tech makers',\n",
    "                  'makedonian': 'macedonian', 'makeschool': 'make school', 'anxietymake': 'anxiety make',\n",
    "                  'makeshifter': 'make shifter', 'countryball': 'country ball', 'Whichcountry': 'Which country',\n",
    "                  'countryHow': 'country How', 'Zenfone': 'Zen fone', 'Electroneum': 'Electro neum',\n",
    "                  'electroneum': 'electro neum', 'Demonetisation': 'demonetization', 'zenfone': 'zen fone',\n",
    "                  'ZenFone': 'Zen Fone', 'onecoin': 'one coin', 'demonetizing': 'demonetized',\n",
    "                  'iphone7': 'iPhone', 'iPhone6': 'iPhone', 'microneedling': 'micro needling', 'iphone6': 'iPhone',\n",
    "                  'Monegasques': 'Monegasque s', 'demonetised': 'demonetized',\n",
    "                  'EveryoneDiesTM': 'EveryoneDies TM', 'teststerone': 'testosterone', 'DoneDone': 'Done Done',\n",
    "                  'papermoney': 'paper money', 'Sasabone': 'Sasa bone', 'Blackphone': 'Black phone',\n",
    "                  'Bonechiller': 'Bone chiller', 'Moneyfront': 'Money front', 'workdone': 'work done',\n",
    "                  'iphoneX': 'iPhone', 'roxycodone': 'r oxycodone',\n",
    "                  'moneycard': 'money card', 'Fantocone': 'Fantocine', 'eletronegativity': 'electronegativity',\n",
    "                  'mellophones': 'mellophone s', 'isotones': 'iso tones', 'donesnt': 'doesnt',\n",
    "                  'thereanyone': 'there anyone', 'electronegativty': 'electronegativity',\n",
    "                  'commissiioned': 'commissioned', 'earvphone': 'earphone', 'condtioners': 'conditioners',\n",
    "                  'demonetistaion': 'demonetization', 'ballonets': 'ballo nets', 'DoneClaim': 'Done Claim',\n",
    "                  'alimoney': 'alimony', 'iodopovidone': 'iodo povidone', 'bonesetters': 'bone setters',\n",
    "                  'componendo': 'compon endo', 'probationees': 'probationers', 'one300': 'one 300',\n",
    "                  'nonelectrolyte': 'non electrolyte', 'ozonedepletion': 'ozone depletion',\n",
    "                  'Stonehart': 'Stone hart', 'Vodafone2': 'Vodafones', 'chaparone': 'chaperone',\n",
    "                  'Noonein': 'Noo nein', 'Frosione': 'Erosion', 'IPhone7': 'Iphone', 'pentanone': 'penta none',\n",
    "                  'poneglyphs': 'pone glyphs', 'cyclohexenone': 'cyclohexanone', 'marlstone': 'marls tone',\n",
    "                  'androneda': 'andromeda', 'iphone8': 'iPhone', 'acidtone': 'acid tone',\n",
    "                  'noneconomically': 'non economically', 'Honeyfund': 'Honey fund', 'germanophone': 'Germanophobe',\n",
    "                  'Democratizationed': 'Democratization ed', 'haoneymoon': 'honeymoon', 'iPhone7': 'iPhone 7',\n",
    "                  'someonewith': 'some onewith', 'Hexanone': 'Hexa none', 'bonespur': 'bones pur',\n",
    "                  'sisterzoned': 'sister zoned', 'HasAnyone': 'Has Anyone',\n",
    "                  'stonepelters': 'stone pelters', 'Chronexia': 'Chronaxia', 'brotherzone': 'brother zone',\n",
    "                  'brotherzoned': 'brother zoned', 'fonecare': 'f onecare', 'nonexsistence': 'nonexistence',\n",
    "                  'conents': 'contents', 'phonecases': 'phone cases', 'Commissionerates': 'Commissioner ates',\n",
    "                  'activemoney': 'active money', 'dingtone': 'ding tone', 'wheatestone': 'wheatstone',\n",
    "                  'chiropractorone': 'chiropractor one', 'heeadphones': 'headphones', 'Maimonedes': 'Maimonides',\n",
    "                  'onepiecedeals': 'onepiece deals', 'oneblade': 'one blade', 'venetioned': 'Venetianed',\n",
    "                  'sunnyleone': 'sunny leone', 'prendisone': 'prednisone', 'Anglosaxophone': 'Anglo saxophone',\n",
    "                  'Blackphones': 'Black phones', 'jionee': 'jinnee', 'chromonema': 'chromo nema',\n",
    "                  'iodoketones': 'iodo ketones', 'demonetizations': 'demonetization', 'aomeone': 'someone',\n",
    "                  'trillonere': 'trillones', 'abandonee': 'abandon',\n",
    "                  'MasterColonel': 'Master Colonel', 'fronend': 'friend', 'Wildstone': 'Wilds tone',\n",
    "                  'patitioned': 'petitioned', 'lonewolfs': 'lone wolfs', 'Spectrastone': 'Spectra stone',\n",
    "                  'dishonerable': 'dishonorable', 'poisiones': 'poisons',\n",
    "                  'condioner': 'conditioner', 'unpermissioned': 'unper missioned', 'friedzone': 'fried zone',\n",
    "                  'umumoney': 'umu money', 'anyonestudied': 'anyone studied', 'dictioneries': 'dictionaries',\n",
    "                  'nosebone': 'nose bone', 'ofVodafone': 'of Vodafone',\n",
    "                  'Yumstone': 'Yum stone', 'oxandrolonesteroid': 'oxandrolone steroid',\n",
    "                  'Mifeprostone': 'Mifepristone', 'pheramones': 'pheromones',\n",
    "                  'sinophone': 'Sinophobe', 'peloponesian': 'peloponnesian', 'michrophone': 'microphone',\n",
    "                  'commissionets': 'commissioners', 'methedone': 'methadone', 'cobditioners': 'conditioners',\n",
    "                  'urotone': 'protone', 'smarthpone': 'smartphone', 'conecTU': 'connect you', 'beloney': 'boloney',\n",
    "                  'comfortzone': 'comfort zone', 'testostersone': 'testosterone', 'camponente': 'component',\n",
    "                  'Idonesia': 'Indonesia', 'dolostones': 'dolostone', 'psiphone': 'psi phone',\n",
    "                  'ceftriazone': 'ceftriaxone', 'feelonely': 'feel onely', 'monetation': 'moderation',\n",
    "                  'activationenergy': 'activation energy', 'moneydriven': 'money driven',\n",
    "                  'staionery': 'stationery', 'zoneflex': 'zone flex', 'moneycash': 'money cash',\n",
    "                  'conectiin': 'connection', 'Wannaone': 'Wanna one',\n",
    "                  'Pictones': 'Pict ones', 'demonentization': 'demonetization',\n",
    "                  'phenonenon': 'phenomenon', 'evenafter': 'even after', 'Sevenfriday': 'Seven friday',\n",
    "                  'Devendale': 'Evendale', 'theeventchronicle': 'the event chronicle',\n",
    "                  'seventysomething': 'seventy something', 'sevenpointed': 'seven pointed',\n",
    "                  'richfeel': 'rich feel', 'overfeel': 'over feel', 'feelingstupid': 'feeling stupid',\n",
    "                  'Photofeeler': 'Photo feeler', 'feelomgs': 'feelings', 'feelinfs': 'feelings',\n",
    "                  'PlayerUnknown': 'Player Unknown', 'Playerunknown': 'Player unknown', 'knowlefge': 'knowledge',\n",
    "                  'knowledgd': 'knowledge', 'knowledeg': 'knowledge', 'knowble': 'Knowle', 'Howknow': 'Howk now',\n",
    "                  'knowledgeWoods': 'knowledge Woods', 'knownprogramming': 'known programming',\n",
    "                  'selfknowledge': 'self knowledge', 'knowldage': 'knowledge', 'knowyouve': 'know youve',\n",
    "                  'aknowlege': 'knowledge', 'Audetteknown': 'Audette known', 'knowlegdeable': 'knowledgeable',\n",
    "                  'trueoutside': 'true outside', 'saynthesize': 'synthesize', 'EssayTyper': 'Essay Typer',\n",
    "                  'meesaya': 'mee saya', 'Rasayanam': 'Rasayan am', 'fanessay': 'fan essay', 'momsays': 'moms ays',\n",
    "                  'sayying': 'saying', 'saydaw': 'say daw', 'Fanessay': 'Fan essay', 'theyreally': 'they really',\n",
    "                  'gayifying': 'gayed up with homosexual love', 'gayke': 'gay Online retailers',\n",
    "                  'Lingayatism': 'Lingayat',\n",
    "                  'macapugay': 'Macaulay', 'jewsplain': 'jews plain',\n",
    "                  'banggood': 'bang good', 'goodfriends': 'good friends',\n",
    "                  'goodfirms': 'good firms', 'Banggood': 'Bang good', 'dogooder': 'do gooder',\n",
    "                  'stillshots': 'stills hots', 'stillsuits': 'still suits', 'panromantic': 'pan romantic',\n",
    "                  'paracommando': 'para commando', 'romantize': 'romanize', 'manupulative': 'manipulative',\n",
    "                  'manjha': 'mania', 'mankrit': 'mank rit',\n",
    "                  'heteroromantic': 'hetero romantic', 'pulmanery': 'pulmonary', 'manpads': 'man pads',\n",
    "                  'supermaneuverable': 'super maneuverable', 'mandatkry': 'mandatory', 'armanents': 'armaments',\n",
    "                  'manipative': 'mancipative', 'himanity': 'humanity', 'maneuever': 'maneuver',\n",
    "                  'Kumarmangalam': 'Kumar mangalam', 'Brahmanwadi': 'Brahman wadi',\n",
    "                  'exserviceman': 'ex serviceman',\n",
    "                  'managewp': 'managed', 'manies': 'many', 'recordermans': 'recorder mans',\n",
    "                  'Feymann': 'Heymann', 'salemmango': 'salem mango', 'manufraturing': 'manufacturing',\n",
    "                  'sreeman': 'freeman', 'tamanaa': 'Tamanac', 'chlamydomanas': 'chlamydomonas',\n",
    "                  'comandant': 'commandant', 'huemanity': 'humanity', 'manaagerial': 'managerial',\n",
    "                  'lithromantics': 'lith romantics',\n",
    "                  'geramans': 'germans', 'Nagamandala': 'Naga mandala', 'humanitariarism': 'humanitarianism',\n",
    "                  'wattman': 'watt man', 'salesmanago': 'salesman ago', 'Washwoman': 'Wash woman',\n",
    "                  'rammandir': 'ram mandir', 'nomanclature': 'nomenclature', 'Haufman': 'Kaufman',\n",
    "                  'prefomance': 'performance', 'ramanunjan': 'Ramanujan', 'Freemansonry': 'Freemasonry',\n",
    "                  'supermaneuverability': 'super maneuverability', 'manstruate': 'menstruate',\n",
    "                  'Tarumanagara': 'Taruma nagara', 'RomanceTale': 'Romance Tale', 'heteromantic': 'hete romantic',\n",
    "                  'terimanals': 'terminals', 'womansplaining': 'wo mansplaining',\n",
    "                  'performancelearning': 'performance learning', 'sociomantic': 'sciomantic',\n",
    "                  'batmanvoice': 'batman voice', 'PerformanceTesting': 'Performance Testing',\n",
    "                  'manorialism': 'manorial ism', 'newscommando': 'news commando',\n",
    "                  'Entwicklungsroman': 'Entwicklungs roman',\n",
    "                  'Kunstlerroman': 'Kunstler roman', 'bodhidharman': 'Bodhidharma', 'Howmaney': 'How maney',\n",
    "                  'manufucturing': 'manufacturing', 'remmaning': 'remaining', 'rangeman': 'range man',\n",
    "                  'mythomaniac': 'mythomania', 'katgmandu': 'katmandu',\n",
    "                  'Superowoman': 'Superwoman', 'Rahmanland': 'Rahman land', 'Dormmanu': 'Dormant',\n",
    "                  'Geftman': 'Gentman', 'manufacturig': 'manufacturing', 'bramanistic': 'Brahmanistic',\n",
    "                  'padmanabhanagar': 'padmanabhan agar', 'homoromantic': 'homo romantic', 'femanists': 'feminists',\n",
    "                  'demihuman': 'demi human', 'manrega': 'Manresa', 'Pasmanda': 'Pas manda',\n",
    "                  'manufacctured': 'manufactured', 'remaninder': 'remainder', 'Marimanga': 'Mari manga',\n",
    "                  'Sloatman': 'Sloat man', 'manlet': 'man let', 'perfoemance': 'performance',\n",
    "                  'mangolian': 'mongolian', 'mangekyu': 'mange kyu', 'mansatory': 'mandatory',\n",
    "                  'managemebt': 'management', 'manufctures': 'manufactures', 'Bramanical': 'Brahmanical',\n",
    "                  'manaufacturing': 'manufacturing', 'Lakhsman': 'Lakhs man', 'Sarumans': 'Sarum ans',\n",
    "                  'mangalasutra': 'mangalsutra', 'Germanised': 'German ised',\n",
    "                  'managersworking': 'managers working', 'cammando': 'commando', 'mandrillaris': 'mandrill aris',\n",
    "                  'Emmanvel': 'Emmarvel', 'manupalation': 'manipulation', 'welcomeromanian': 'welcome romanian',\n",
    "                  'humanfemale': 'human female', 'mankirt': 'mankind', 'Haffmann': 'Hoffmann',\n",
    "                  'Panromantic': 'Pan romantic', 'demantion': 'detention', 'Suparwoman': 'Superwoman',\n",
    "                  'parasuramans': 'parasuram ans', 'sulmann': 'Suilmann', 'Shubman': 'Subman',\n",
    "                  'manspread': 'man spread', 'mandingan': 'Mandingan', 'mandalikalu': 'mandalika lu',\n",
    "                  'manufraturer': 'manufacturer', 'Wedgieman': 'Wedgie man', 'manwues': 'manages',\n",
    "                  'humanzees': 'human zees', 'Steymann': 'Stedmann', 'Jobberman': 'Jobber man',\n",
    "                  'maniquins': 'mani quins', 'biromantical': 'bi romantical', 'Rovman': 'Roman',\n",
    "                  'pyromantic': 'pyro mantic', 'Tastaman': 'Rastaman', 'Spoolman': 'Spool man',\n",
    "                  'Subramaniyan': 'Subramani yan', 'abhimana': 'abhiman a', 'manholding': 'man holding',\n",
    "                  'seviceman': 'serviceman', 'womansplained': 'womans plained', 'manniya': 'mania',\n",
    "                  'Bhraman': 'Braman', 'Laakman': 'Layman', 'mansturbate': 'masturbate',\n",
    "                  'Sulamaniya': 'Sulamani ya', 'demanters': 'decanters', 'postmanare': 'postman are',\n",
    "                  'mannualy': 'annual', 'rstman': 'Rotman', 'permanentjobs': 'permanent jobs',\n",
    "                  'Allmang': 'All mang', 'TradeCommander': 'Trade Commander', 'BasedStickman': 'Based Stickman',\n",
    "                  'Deshabhimani': 'Desha bhimani', 'manslamming': 'mans lamming', 'Brahmanwad': 'Brahman wad',\n",
    "                  'fundemantally': 'fundamentally', 'supplemantary': 'supplementary', 'egomanias': 'ego manias',\n",
    "                  'manvantar': 'Manvantara', 'spymania': 'spy mania', 'mangonada': 'mango nada',\n",
    "                  'manthras': 'mantras', 'Humanpark': 'Human park', 'manhuas': 'mahuas',\n",
    "                  'manterrupting': 'interrupting', 'dermatillomaniac': 'dermatillomania',\n",
    "                  'performancies': 'performances', 'manipulant': 'manipulate',\n",
    "                  'painterman': 'painter man', 'mangalik': 'manglik',\n",
    "                  'Neurosemantics': 'Neuro semantics', 'discrimantion': 'discrimination',\n",
    "                  'Womansplaining': 'feminist', 'mongodump': 'mongo dump', 'roadgods': 'road gods',\n",
    "                  'Oligodendraglioma': 'Oligodendroglioma', 'unrightly': 'un rightly', 'Janewright': 'Jane wright',\n",
    "                  ' righten ': ' tighten ', 'brightiest': 'brightest',\n",
    "                  'frighter': 'fighter', 'righteouness': 'righteousness', 'triangleright': 'triangle right',\n",
    "                  'Brightspace': 'Brights pace', 'techinacal': 'technical', 'chinawares': 'china wares',\n",
    "                  'Vancouever': 'Vancouver', 'cheverlet': 'cheveret', 'deverstion': 'diversion',\n",
    "                  'everbodys': 'everybody', 'Dramafever': 'Drama fever', 'reverificaton': 'reverification',\n",
    "                  'canterlever': 'canter lever', 'keywordseverywhere': 'keywords everywhere',\n",
    "                  'neverunlearned': 'never unlearned', 'everyfirst': 'every first',\n",
    "                  'neverhteless': 'nevertheless', 'clevercoyote': 'clever coyote', 'irrevershible': 'irreversible',\n",
    "                  'achievership': 'achievers hip', 'easedeverything': 'eased everything', 'youbever': 'you bever',\n",
    "                  'everperson': 'ever person', 'everydsy': 'everyday', 'whemever': 'whenever',\n",
    "                  'everyonr': 'everyone', 'severiity': 'severity', 'narracist': 'nar racist',\n",
    "                  'racistly': 'racist', 'takesuch': 'take such', 'mystakenly': 'mistakenly',\n",
    "                  'shouldntake': 'shouldnt take', 'Kalitake': 'Kali take', 'msitake': 'mistake',\n",
    "                  'straitstimes': 'straits times', 'timefram': 'timeframe', 'watchtime': 'watch time',\n",
    "                  'timetraveling': 'timet raveling', 'peactime': 'peacetime', 'timetabe': 'timetable',\n",
    "                  'cooktime': 'cook time', 'blocktime': 'block time', 'timesjobs': 'times jobs',\n",
    "                  'timesence': 'times ence', 'Touchtime': 'Touch time', 'timeloop': 'time loop',\n",
    "                  'subcentimeter': 'sub centimeter', 'timejobs': 'time jobs', 'Guardtime': 'Guard time',\n",
    "                  'realtimepolitics': 'realtime politics', 'loadingtimes': 'loading times',\n",
    "                  'timesnow': '24-hour English news channel in India', 'timesspark': 'times spark',\n",
    "                  'timetravelling': 'timet ravelling',\n",
    "                  'antimeter': 'anti meter', 'timewaste': 'time waste', 'cryptochristians': 'crypto christians',\n",
    "                  'Whatcould': 'What could', 'becomesdouble': 'becomes double', 'deathbecomes': 'death becomes',\n",
    "                  'youbecome': 'you become', 'greenseer': 'people who possess the magical ability',\n",
    "                  'rseearch': 'research', 'homeseek': 'home seek',\n",
    "                  'Greenseer': 'people who possess the magical ability', 'starseeders': 'star seeders',\n",
    "                  'seekingmillionaire': 'seeking millionaire', 'see\\u202c': 'see',\n",
    "                  'seeies': 'series', 'CodeAgon': 'Code Agon',\n",
    "                  'royago': 'royal', 'Dragonkeeper': 'Dragon keeper', 'mcgreggor': 'McGregor',\n",
    "                  'catrgory': 'category', 'Dragonknight': 'Dragon knight', 'Antergos': 'Anteros',\n",
    "                  'togofogo': 'togo fogo', 'mongorestore': 'mongo restore', 'gorgops': 'gorgons',\n",
    "                  'withgoogle': 'with google', 'goundar': 'Gondar', 'algorthmic': 'algorithmic',\n",
    "                  'goatnuts': 'goat nuts', 'vitilgo': 'vitiligo', 'polygony': 'poly gony',\n",
    "                  'digonals': 'diagonals', 'Luxemgourg': 'Luxembourg', 'UCSanDiego': 'UC SanDiego',\n",
    "                  'Ringostat': 'Ringo stat', 'takingoff': 'taking off', 'MongoImport': 'Mongo Import',\n",
    "                  'alggorithms': 'algorithms', 'dragonknight': 'dragon knight', 'negotiatior': 'negotiation',\n",
    "                  'gomovies': 'go movies', 'Withgott': 'Without',\n",
    "                  'categoried': 'categories', 'Stocklogos': 'Stock logos', 'Pedogogical': 'Pedological',\n",
    "                  'Wedugo': 'Wedge', 'golddig': 'gold dig', 'goldengroup': 'golden group',\n",
    "                  'merrigo': 'merligo', 'googlemapsAPI': 'googlemaps API', 'goldmedal': 'gold medal',\n",
    "                  'golemized': 'polemized', 'Caligornia': 'California', 'unergonomic': 'un ergonomic',\n",
    "                  'fAegon': 'wagon', 'vertigos': 'vertigo s', 'trigonomatry': 'trigonometry',\n",
    "                  'hypogonadic': 'hypogonadia', 'Mogolia': 'Mongolia', 'governmaent': 'government',\n",
    "                  'ergotherapy': 'ergo therapy', 'Bogosort': 'Bogo sort', 'goalwise': 'goal wise',\n",
    "                  'alogorithms': 'algorithms', 'MercadoPago': 'Mercado Pago', 'rivigo': 'rivi go',\n",
    "                  'govshutdown': 'gov shutdown', 'gorlfriend': 'girlfriend',\n",
    "                  'stategovt': 'state govt', 'Chickengonia': 'Chicken gonia', 'Yegorovich': 'Yegorov ich',\n",
    "                  'regognitions': 'recognitions', 'gorichen': 'Gori Chen Mountain',\n",
    "                  'goegraphies': 'geographies', 'gothras': 'goth ras', 'belagola': 'bela gola',\n",
    "                  'snapragon': 'snapdragon', 'oogonial': 'oogonia l', 'Amigofoods': 'Amigo foods',\n",
    "                  'Sigorn': 'son of Styr', 'algorithimic': 'algorithmic',\n",
    "                  'innermongolians': 'inner mongolians', 'ArangoDB': 'Arango DB', 'zigolo': 'gigolo',\n",
    "                  'regognized': 'recognized', 'Moongot': 'Moong ot', 'goldquest': 'gold quest',\n",
    "                  'catagorey': 'category', 'got7': 'got', 'jetbingo': 'jet bingo', 'Dragonchain': 'Dragon chain',\n",
    "                  'catwgorized': 'categorized', 'gogoro': 'gogo ro', 'Tobagoans': 'Tobago ans',\n",
    "                  'digonal': 'di gonal', 'algoritmic': 'algorismic', 'dragonflag': 'dragon flag',\n",
    "                  'Indigoflight': 'Indigo flight',\n",
    "                  'governening': 'governing', 'ergosphere': 'ergo sphere',\n",
    "                  'pingo5': 'pingo', 'Montogo': 'montego', 'Rivigo': 'technology-enabled logistics company',\n",
    "                  'Jigolo': 'Gigolo', 'phythagoras': 'pythagoras', 'Mangolian': 'Mongolian',\n",
    "                  'forgottenfaster': 'forgotten faster', 'stargold': 'a Hindi movie channel',\n",
    "                  'googolplexain': 'googolplexian', 'corpgov': 'corp gov',\n",
    "                  'govtribe': 'provides real-time federal contracting market intel',\n",
    "                  'dragonglass': 'dragon glass', 'gorakpur': 'Gorakhpur', 'MangoPay': 'Mango Pay',\n",
    "                  'chigoe': 'sub-tropical climates', 'BingoBox': 'an investment company', '走go': 'go',\n",
    "                  'followingorder': 'following order', 'pangolinminer': 'pangolin miner',\n",
    "                  'negosiation': 'negotiation', 'lexigographers': 'lexicographers', 'algorithom': 'algorithm',\n",
    "                  'unforgottable': 'unforgettable', 'wellsfargoemail': 'wellsfargo email',\n",
    "                  'daigonal': 'diagonal', 'Pangoro': 'cantankerous Pokemon', 'negotiotions': 'negotiations',\n",
    "                  'Swissgolden': 'Swiss golden', 'google4': 'google', 'Agoraki': 'Ago raki',\n",
    "                  'Garthago': 'Carthago', 'Stegosauri': 'stegosaurus', 'ergophobia': 'ergo phobia',\n",
    "                  'bigolive': 'big olive', 'bittergoat': 'bitter goat', 'naggots': 'faggots',\n",
    "                  'googology': 'online encyclopedia', 'algortihms': 'algorithms', 'bengolis': 'Bengalis',\n",
    "                  'fingols': 'Finnish people are supposedly descended from Mongols',\n",
    "                  'savethechildren': 'save thechildren',\n",
    "                  'stopings': 'stoping', 'stopsits': 'stop sits', 'stopsigns': 'stop signs',\n",
    "                  'Galastop': 'Galas top', 'pokestops': 'pokes tops', 'forcestop': 'forces top',\n",
    "                  'Hopstop': 'Hops top', 'stoppingexercises': 'stopping exercises', 'coinstop': 'coins top',\n",
    "                  'stoppef': 'stopped', 'workaway': 'work away', 'snazzyway': 'snazzy way',\n",
    "                  'Rewardingways': 'Rewarding ways', 'cloudways': 'cloud ways', 'Cloudways': 'Cloud ways',\n",
    "                  'Brainsway': 'Brains way', 'nesraway': 'nearaway',\n",
    "                  'AlwaysHired': 'Always Hired', 'expessway': 'expressway', 'Syncway': 'Sync way',\n",
    "                  'LeewayHertz': 'Blockchain Company', 'towayrds': 'towards', 'swayable': 'sway able',\n",
    "                  'Telloway': 'Tello way', 'palsmodium': 'plasmodium', 'Gobackmodi': 'Goback modi',\n",
    "                  'comodies': 'corodies', 'islamphobic': 'islam phobic', 'islamphobia': 'islam phobia',\n",
    "                  'citiesbetter': 'cities better', 'betterv3': 'better', 'betterDtu': 'better Dtu',\n",
    "                  'Babadook': 'a horror drama film', 'Ahemadabad': 'Ahmadabad', 'faidabad': 'Faizabad',\n",
    "                  'Amedabad': 'Ahmedabad', 'kabadii': 'kabaddi', 'badmothing': 'badmouthing',\n",
    "                  'badminaton': 'badminton', 'badtameezdil': 'badtameez dil', 'badeffects': 'bad effects',\n",
    "                  '∠bad': 'bad', 'ahemadabad': 'Ahmadabad', 'embaded': 'embased', 'Isdhanbad': 'Is dhanbad',\n",
    "                  'badgermoles': 'enormous, blind mammal', 'allhabad': 'Allahabad', 'ghazibad': 'ghazi bad',\n",
    "                  'htderabad': 'Hyderabad', 'Auragabad': 'Aurangabad', 'ahmedbad': 'Ahmedabad',\n",
    "                  'ahmdabad': 'Ahmadabad', 'alahabad': 'Allahabad',\n",
    "                  'Hydeabad': 'Hyderabad', 'Gyroglove': 'wearable technology', 'foodlovee': 'food lovee',\n",
    "                  'slovenised': 'slovenia', 'handgloves': 'hand gloves', 'lovestep': 'love step',\n",
    "                  'lovejihad': 'love jihad', 'RolloverBox': 'Rollover Box', 'stupidedt': 'stupidest',\n",
    "                  'toostupid': 'too stupid',\n",
    "                  'pakistanisbeautiful': 'pakistanis beautiful', 'ispakistan': 'is pakistan',\n",
    "                  'inpersonations': 'impersonations', 'medicalperson': 'medical person',\n",
    "                  'interpersonation': 'inter personation', 'workperson': 'work person',\n",
    "                  'personlich': 'person lich', 'persoenlich': 'person lich',\n",
    "                  'middleperson': 'middle person', 'personslized': 'personalized',\n",
    "                  'personifaction': 'personification', 'welcomemarriage': 'welcome marriage',\n",
    "                  'come2': 'come to', 'upcomedians': 'up comedians', 'overvcome': 'overcome',\n",
    "                  'talecome': 'tale come', 'cometitive': 'competitive', 'arencome': 'aren come',\n",
    "                  'achecomes': 'ache comes', '」come': 'come',\n",
    "                  'comepleted': 'completed', 'overcomeanxieties': 'overcome anxieties',\n",
    "                  'demigirl': 'demi girl', 'gridgirl': 'female models of the race', 'halfgirlfriend': 'half girlfriend',\n",
    "                  'girlriend': 'girlfriend', 'fitgirl': 'fit girl', 'girlfrnd': 'girlfriend', 'awrong': 'aw rong',\n",
    "                  'northcap': 'north cap', 'productionsupport': 'production support',\n",
    "                  'Designbold': 'Online Photo Editor Design Studio',\n",
    "                  'skyhold': 'sky hold', 'shuoldnt': 'shouldnt', 'anarold': 'Android', 'yaerold': 'year old',\n",
    "                  'soldiders': 'soldiers', 'indrold': 'Android', 'blindfoldedly': 'blindfolded',\n",
    "                  'overcold': 'over cold', 'Goldmont': 'microarchitecture in Intel', 'boldspot': 'bolds pot',\n",
    "                  'Rankholders': 'Rank holders', 'cooldrink': 'cool drink', 'beltholders': 'belt holders',\n",
    "                  'GoldenDict': 'open-source dictionary program', 'softskill': 'softs kill',\n",
    "                  'Cooldige': 'the 30th president of the United States',\n",
    "                  'newkiller': 'new killer', 'skillselect': 'skills elect', 'nonskilled': 'non skilled',\n",
    "                  'killyou': 'kill you', 'Skillport': 'Army e-Learning Program', 'unkilled': 'un killed',\n",
    "                  'killikng': 'killing', 'killograms': 'kilograms',\n",
    "                  'Worldkillers': 'World killers', 'reskilled': 'skilled',\n",
    "                  'killedshivaji': 'killed shivaji', 'honorkillings': 'honor killings',\n",
    "                  'skillclasses': 'skill classes', 'microskills': 'micros kills',\n",
    "                  'Skillselect': 'Skills elect', 'ratkill': 'rat kill',\n",
    "                  'pleasegive': 'please give', 'flashgive': 'flash give',\n",
    "                  'southerntelescope': 'southern telescope', 'westsouth': 'west south',\n",
    "                  'southAfricans': 'south Africans', 'Joboutlooks': 'Job outlooks', 'joboutlook': 'job outlook',\n",
    "                  'Outlook365': 'Outlook 365', 'Neulife': 'Neu life', 'qualifeid': 'qualified',\n",
    "                  'nullifed': 'nullified', 'lifeaffect': 'life affect', 'lifestly': 'lifestyle',\n",
    "                  'aristocracylifestyle': 'aristocracy lifestyle', 'antilife': 'anti life',\n",
    "                  'afterafterlife': 'after afterlife', 'lifestylye': 'lifestyle', 'prelife': 'pre life',\n",
    "                  'lifeute': 'life ute', 'liferature': 'literature',\n",
    "                  'securedlife': 'secured life', 'doublelife': 'double life', 'antireligion': 'anti religion',\n",
    "                  'coreligionist': 'co religionist', 'petrostates': 'petro states', 'otherstates': 'others tates',\n",
    "                  'spacewithout': 'space without', 'withoutyou': 'without you',\n",
    "                  'withoutregistered': 'without registered', 'weightwithout': 'weight without',\n",
    "                  'withoutcheck': 'without check', 'milkwithout': 'milk without',\n",
    "                  'Highschoold': 'High school', 'memoney': 'money', 'moneyof': 'mony of', 'Oneplus': 'OnePlus',\n",
    "                  'OnePlus': 'Chinese smartphone manufacturer', 'Beerus': 'the God of Destruction',\n",
    "                  'takeoverr': 'takeover', 'demonetizedd': 'demonetized', 'polyhouse': 'Polytunnel',\n",
    "                  'Elitmus': 'eLitmus', 'eLitmus': 'Indian company that helps companies in hiring employees',\n",
    "                  'becone': 'become', 'nestaway': 'nest away', 'takeoverrs': 'takeovers', 'Istop': 'I stop',\n",
    "                  'Austira': 'Australia', 'germeny': 'Germany', 'mansoon': 'man soon',\n",
    "                  'worldmax': 'wholesaler of drum parts',\n",
    "                  'ammusement': 'amusement', 'manyare': 'many are', 'supplymentary': 'supply mentary',\n",
    "                  'timesup': 'times up', 'homologus': 'homologous', 'uimovement': 'ui movement', 'spause': 'spouse',\n",
    "                  'aesexual': 'asexual', 'Iovercome': 'I overcome', 'developmeny': 'development',\n",
    "                  'hindusm': 'hinduism', 'sexpat': 'sex tourism', 'sunstop': 'sun stop', 'polyhouses': 'Polytunnel',\n",
    "                  'usefl': 'useful', 'Fundamantal': 'fundamental', 'environmentai': 'environmental',\n",
    "                  'Redmi': 'Xiaomi Mobile', 'Loy Machedo': ' Motivational Speaker ', 'unacademy': 'Unacademy',\n",
    "                  'Boruto': 'Naruto Next Generations', 'Upwork': 'Up work',\n",
    "                  'Unacademy': 'educational technology company',\n",
    "                  'HackerRank': 'Hacker Rank', 'upwork': 'up work', 'Chromecast': 'Chrome cast',\n",
    "                  'microservices': 'micro services', 'Undertale': 'video game', 'undergraduation': 'under graduation',\n",
    "                  'chapterwise': 'chapter wise', 'twinflame': 'twin flame', 'Hotstar': 'Hot star',\n",
    "                  'blockchains': 'blockchain',\n",
    "                  'darkweb': 'dark web', 'Microservices': 'Micro services', 'Nearbuy': 'Nearby',\n",
    "                  ' Padmaavat ': ' Padmavati ', ' padmavat ': ' Padmavati ', ' Padmaavati ': ' Padmavati ',\n",
    "                  ' Padmavat ': ' Padmavati ', ' internshala ': ' internship and online training platform in India ',\n",
    "                  'dream11': ' fantasy sports platform in India ', 'conciousnesss': 'consciousnesses',\n",
    "                  'Dream11': ' fantasy sports platform in India ', 'cointry': 'country', ' coinvest ': ' invest ',\n",
    "                  '23 andme': 'privately held personal genomics and biotechnology company in California',\n",
    "                  'Trumpism': 'philosophy and politics espoused by Donald Trump',\n",
    "                  'Trumpian': 'viewpoints of President Donald Trump', 'Trumpists': 'admirer of Donald Trump',\n",
    "                  'coincidents': 'coincidence', 'coinsized': 'coin sized', 'coincedences': 'coincidences',\n",
    "                  'cointries': 'countries', 'coinsidered': 'considered', 'coinfirm': 'confirm',\n",
    "                  'humilates':'humiliates', 'vicevice':'vice vice', 'politicak':'political', 'Sumaterans':'Sumatrans',\n",
    "                  'Kamikazis':'Kamikazes', 'unmoraled':'unmoral', 'eduacated':'educated', 'moraled':'morale',\n",
    "                  'Amharc':'Amarc', 'where Burkhas':'wear Burqas', 'Baloochistan':'Balochistan', 'durgahs':'durgans',\n",
    "                  'illigitmate':'illegitimate', 'hillum':'helium','treatens':'threatens','mutiliating':'mutilating',\n",
    "                  'speakingly':'speaking', 'pretex':'pretext', 'menstruateion':'menstruation', \n",
    "                  'genocidizing':'genociding', 'maratis':'Maratism','Parkistinian':'Pakistani', 'SPEICIAL':'SPECIAL',\n",
    "                  'REFERNECE':'REFERENCE', 'provocates':'provokes', 'FAMINAZIS':'FEMINAZIS', 'repugicans':'republicans',\n",
    "                  'tonogenesis':'tone', 'winor':'win', 'redicules':'ridiculous', 'Beluchistan':'Balochistan', \n",
    "                  'volime':'volume', 'namaj':'namaz', 'CONgressi':'Congress', 'Ashifa':'Asifa', 'queffing':'queefing',\n",
    "                  'montheistic':'nontheistic', 'Rajsthan':'Rajasthan', 'Rajsthanis':'Rajasthanis', 'specrum':'spectrum',\n",
    "                  'brophytes':'bryophytes', 'adhaar':'Adhara', 'slogun':'slogan', 'harassd':'harassed',\n",
    "                  'transness':'trans gender', 'Insdians':'Indians', 'Trampaphobia':'Trump aphobia', 'attrected':'attracted',\n",
    "                  'Yahtzees':'Yahtzee', 'thiests':'atheists', 'thrir':'their', 'extraterestrial':'extraterrestrial',\n",
    "                  'silghtest':'slightest', 'primarty':'primary','brlieve':'believe', 'fondels':'fondles',\n",
    "                  'loundly':'loudly', 'bootythongs':'booty thongs', 'understamding':'understanding', 'degenarate':'degenerate',\n",
    "                  'narsistic':'narcistic', 'innerskin':'inner skin','spectulated':'speculated', 'hippocratical':'Hippocratical',\n",
    "                  'itstead':'instead', 'parralels':'parallels', 'sloppers':'slippers'\n",
    "                  }\n",
    "\n",
    "def clean_bad_case_words(text):\n",
    "    for bad_word in bad_case_words:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, bad_case_words[bad_word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.61 ms\n"
     ]
    }
   ],
   "source": [
    "mis_connect_list = ['(W|w)hat', '(W|w)hy', '(H|h)ow', '(W|w)hich', '(W|w)here', '(W|w)ill']\n",
    "mis_connect_re = re.compile('(%s)' % '|'.join(mis_connect_list))\n",
    "\n",
    "mis_spell_mapping = {'whattsup': 'WhatsApp', 'whatasapp':'WhatsApp', 'whatsupp':'WhatsApp', \n",
    "                      'whatcus':'what cause', 'arewhatsapp': 'are WhatsApp', 'Hwhat':'what',\n",
    "                      'Whwhat': 'What', 'whatshapp':'WhatsApp', 'howhat':'how that',\n",
    "                      # why\n",
    "                      'Whybis':'Why is', 'laowhy86':'Foreigners who do not respect China',\n",
    "                      'Whyco-education':'Why co-education',\n",
    "                      # How\n",
    "                      \"Howddo\":\"How do\", 'Howeber':'However', 'Showh':'Show',\n",
    "                      \"Willowmagic\":'Willow magic', 'WillsEye':'Will Eye', 'Williby':'will by'}\n",
    "def spacing_some_connect_words(text):\n",
    "    \"\"\"\n",
    "    'Whyare' -> 'Why are'\n",
    "    \"\"\"\n",
    "    ori = text\n",
    "    for error in mis_spell_mapping:\n",
    "        if error in text:\n",
    "            text = text.replace(error, mis_spell_mapping[error])\n",
    "            \n",
    "    # what\n",
    "    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n",
    "    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n",
    "    # why\n",
    "    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n",
    "    # How\n",
    "    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n",
    "    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n",
    "    # which\n",
    "    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n",
    "    # where\n",
    "    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n",
    "    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n",
    "    # \n",
    "    text = mis_connect_re.sub(r\" \\1 \", text)\n",
    "    text = text.replace(\"What sApp\", 'WhatsApp')\n",
    "    \n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.72 ms\n"
     ]
    }
   ],
   "source": [
    "# clean repeated letters\n",
    "def clean_repeat_words(text):\n",
    "    text = text.replace(\"img\", \"ing\")\n",
    "\n",
    "    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n",
    "    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n",
    "    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n",
    "    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n",
    "    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n",
    "    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n",
    "    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n",
    "    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n",
    "    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n",
    "    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n",
    "    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n",
    "    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n",
    "    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n",
    "    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n",
    "    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n",
    "    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n",
    "    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n",
    "    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n",
    "    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n",
    "    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n",
    "    text = re.sub(r\"plzz+\", \"please\", text)\n",
    "    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.23 ms\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    preprocess text main steps\n",
    "    \"\"\"\n",
    "    text = remove_space(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    text = clean_number(text)\n",
    "    text = pre_clean_rare_words(text)\n",
    "    text = decontracted(text)\n",
    "    text = clean_latex(text)\n",
    "    text = clean_misspell(text)\n",
    "    text = spacing_punctuation(text)\n",
    "    text = spacing_some_connect_words(text)\n",
    "    text = clean_bad_case_words(text)\n",
    "    text = clean_repeat_words(text)\n",
    "    text = remove_space(text)\n",
    "    return text\n",
    "\n",
    "def text_clean_wrapper(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(preprocess)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "train_df = df_parallelize_run(train_df, text_clean_wrapper)\n",
    "test_df = df_parallelize_run(test_df, text_clean_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 72.59% of vocab\n",
      "Found embeddings for  99.44% of all text\n",
      "time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train_df.drop('target', axis=1), test_df]).reset_index(drop=True)\n",
    "vocab = build_vocab(df['question_text'])\n",
    "\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "oov_glove = {\"oov_rate\": len(oov_glove) / len(vocab), 'oov_words': oov_glove}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362492/1362492 [00:07<00:00, 192032.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 18127 bug words\n",
      "time: 7.81 s\n"
     ]
    }
   ],
   "source": [
    "# get current vocabulary, find the words that has '-'\n",
    "cur_vocabulary = set()\n",
    "for text in tqdm(train_df['question_text'].values.tolist() + test_df['question_text'].values.tolist()):\n",
    "    words = text.split(' ')\n",
    "    cur_vocabulary.update(set(words))\n",
    "\n",
    "bug_punc_spacing_words_mapping = {}\n",
    "for vocab in cur_vocabulary:\n",
    "    if '-' in vocab:\n",
    "        # whether the glove or para contain this word\n",
    "        if (vocab in embed_glove or vocab.capitalize() in embed_glove or vocab.lower() in embed_glove):\n",
    "            bug_punc_spacing_words_mapping[\" \" + ' - '.join(vocab.split('-')) + \" \"] =  \" \" + vocab + \" \"\n",
    "    \n",
    "    if '.' in vocab:\n",
    "        if vocab.endswith('.'):\n",
    "            continue\n",
    "        \n",
    "        if (vocab in embed_glove or vocab.capitalize() in embed_glove or vocab.lower() in embed_glove):\n",
    "            bug_punc_spacing_words_mapping[\" \" + ' . '.join(vocab.split('.')) + \" \"] = \" \" + vocab + \" \"\n",
    "                                    \n",
    "del bug_punc_spacing_words_mapping['  -  ']\n",
    "print(\"found %s bug words\" % len(bug_punc_spacing_words_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "def spacing_dash_point(text):\n",
    "    if '-' in text:\n",
    "        text = text.replace('-', ' - ')\n",
    "    if '.' in text:\n",
    "        text = text.replace('.', ' . ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:02<00:00, 553474.88it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 627722.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(spacing_dash_point)\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(spacing_dash_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.33 ms\n"
     ]
    }
   ],
   "source": [
    "def fix_dash_point_spacing_bug(text):\n",
    "    for bug_dash in bug_punc_spacing_words_mapping:\n",
    "        if bug_dash in text:\n",
    "            text = text.replace(bug_dash, bug_punc_spacing_words_mapping[bug_dash])\n",
    "    return text\n",
    "\n",
    "def fix_dash_point_spacing_bug_wrapper(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(fix_dash_point_spacing_bug)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8min 15s\n"
     ]
    }
   ],
   "source": [
    "train_df = df_parallelize_run(train_df, fix_dash_point_spacing_bug_wrapper)\n",
    "test_df = df_parallelize_run(test_df, fix_dash_point_spacing_bug_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.69 ms\n"
     ]
    }
   ],
   "source": [
    "# Convert text to appropriate casing to maximize coverage\n",
    "def modify_casing(text):\n",
    "    replaced_text = []\n",
    "    for word in text.split(' '):\n",
    "        if word in embed_glove:\n",
    "            replaced_text.append(word)\n",
    "        elif word.capitalize() in embed_glove:\n",
    "            replaced_text.append(word.capitalize())\n",
    "        elif word.lower() in embed_glove:\n",
    "            replaced_text.append(word.lower())\n",
    "        elif word.upper() in embed_glove:\n",
    "            replaced_text.append(word.upper())\n",
    "        else:\n",
    "            replaced_text.append(word)\n",
    "    return \" \".join(replaced_text)\n",
    "\n",
    "def remove_weird_ones(text):\n",
    "    text = re.sub('√', '', text)\n",
    "    text = re.sub('°', '', text)\n",
    "    text = re.sub('×', '', text)\n",
    "    text = re.sub('£', '', text)\n",
    "    text = re.sub('π', '', text)\n",
    "    text = re.sub('€', '', text)\n",
    "    text = re.sub('₹', '', text)\n",
    "    text = re.sub('®', '', text)\n",
    "    text = re.sub('¿', '', text)\n",
    "    text = re.sub('÷', '', text)\n",
    "    text = re.sub('≤', '', text)\n",
    "    text = re.sub('∅', '', text)\n",
    "    text = re.sub('∫', '', text)\n",
    "    text = re.sub('α', '', text)\n",
    "    # Replace weird type of crypto-coins\n",
    "    text = re.sub(r'[a-zA-Z]+coin[s]?', 'cryptocurrency', text)\n",
    "    return text\n",
    "    \n",
    "def fix_casing_and_others(df):\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(modify_casing)\n",
    "    df[\"question_text\"] = df[\"question_text\"].apply(remove_weird_ones)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "train_df = df_parallelize_run(train_df, fix_casing_and_others)\n",
    "test_df = df_parallelize_run(test_df, fix_casing_and_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 80.39% of vocab\n",
      "Found embeddings for  99.69% of all text\n",
      "Found embeddings for 74.46% of vocab\n",
      "Found embeddings for  99.56% of all text\n",
      "Found embeddings for 64.13% of vocab\n",
      "Found embeddings for  87.90% of all text\n",
      "time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train_df.drop('target', axis=1), test_df]).reset_index(drop=True)\n",
    "vocab = build_vocab(df['question_text'])\n",
    "\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "oov_glove = {\"oov_rate\": len(oov_glove) / len(vocab), 'oov_words': oov_glove}\n",
    "\n",
    "oov_wiki = check_coverage(vocab, embed_wiki)\n",
    "oov_wiki = {\"oov_rate\": len(oov_wiki) / len(vocab), 'oov_words': oov_wiki}\n",
    "\n",
    "oov_google = check_coverage(vocab, embed_google)\n",
    "oov_google = {\"oov_rate\": len(oov_google) / len(vocab), 'oov_words': oov_google}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "## fill up the missing values\n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X) + list(test_X))\n",
    "\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.55 ms\n"
     ]
    }
   ],
   "source": [
    "def load_glove(word_index, given_embeds):\n",
    "    all_embs = np.stack(given_embeds.values())\n",
    "#     emb_mean, emb_std = np.mean(all_embs), np.std(all_embs)\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = given_embeds.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-2d10653e6e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_matrix_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_matrix_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_wiki\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# embedding_matrix_3 = load_glove(vocab, embed_google)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-061ade03cee2>\u001b[0m in \u001b[0;36mload_glove\u001b[0;34m(word_index, given_embeds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mall_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgiven_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     emb_mean, emb_std = np.mean(all_embs), np.std(all_embs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0memb_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memb_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.005838499\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.48782197\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membed_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_embs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_1 = load_glove(vocab, embed_glove)\n",
    "embedding_matrix_2 = load_glove(vocab, embed_wiki)\n",
    "# embedding_matrix_3 = load_glove(vocab, embed_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = (embedding_matrix_1 + embedding_matrix_2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 580 µs\n"
     ]
    }
   ],
   "source": [
    "assert max_features == embedding_matrix.shape[0]\n",
    "assert embed_size == embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f79081928ca032fbfe3b90c6d3ce91cf57d443d8"
   },
   "source": [
    "**Main part: load, train, pred and blend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxlen is 75\n",
      "Train-data Coverage with 75 : 0.999989\n",
      "Test-data Coverage with 75 : 0.999947\n",
      "time: 333 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Maxlen is %d\" % maxlen)\n",
    "lens = [len(x) for x in train_X]\n",
    "print(\"Train-data Coverage with %d : %f\" % (maxlen,  np.sum(np.array(lens) <= maxlen) / len(train_X)))\n",
    "lens = [len(x) for x in test_X]\n",
    "print(\"Test-data Coverage with %d : %f\" % (maxlen,  np.sum(np.array(lens) <= maxlen) / len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.3 s\n"
     ]
    }
   ],
   "source": [
    "## Pad the sentences\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen, padding='pre', truncating='pre')\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a676c3a275514a3351edf306e02d832a5f39317"
   },
   "source": [
    "**Attention layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_uuid": "84e00df2c7b94205f5588af503f62412c48f46f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.64 ms\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d96793d88c22274d985436e192f62970c227c324"
   },
   "source": [
    "**LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "05164d541a0c35cae727d0338548d156efe21427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.93 ms\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.layers import add\n",
    "\n",
    "def model_lstm_atten(embedding_matrix):\n",
    "    \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.05)(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=0.1, recurrent_dropout=0.1,\n",
    "                           bias_regularizer=l1_l2(0.001, 0.001),\n",
    "                           recurrent_regularizer=l1_l2(0.001, 0.001),))(x)\n",
    "    y = Bidirectional(GRU(32, return_sequences=True, dropout=0.11, recurrent_dropout=0.1,\n",
    "                           bias_regularizer=l1_l2(0.001, 0.001),\n",
    "                           recurrent_regularizer=l1_l2(0.001, 0.001),))(x)\n",
    "    \n",
    "    atten_1 = Attention(maxlen)(x)\n",
    "    atten_2 = Attention(maxlen)(y)\n",
    "    \n",
    "#     conc = concatenate([atten_1, atten_2])\n",
    "    conc = atten_2\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    conc = Dense(32)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = Activation(\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    \n",
    "    conc = Dense(32)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    conc = Activation(\"relu\")(conc)\n",
    "    \n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = train_X[train_indices], train_y[train_indices]\n",
    "X_val, Y_val = train_X[val_indices], train_y[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train individual model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1279998 samples, validate on 26124 samples\n",
      "Epoch 1/20\n",
      "1279998/1279998 [==============================] - 192s 150us/step - loss: 0.5709 - f1: 0.4216 - val_loss: 0.1974 - val_f1: 0.4927\n",
      "Epoch 2/20\n",
      "1279998/1279998 [==============================] - 187s 146us/step - loss: 0.1615 - f1: 0.5256 - val_loss: 0.1297 - val_f1: 0.4987\n",
      "Epoch 3/20\n",
      "1279998/1279998 [==============================] - 186s 146us/step - loss: 0.1208 - f1: 0.5719 - val_loss: 0.1141 - val_f1: 0.5919\n",
      "Epoch 4/20\n",
      "1279998/1279998 [==============================] - 187s 146us/step - loss: 0.1156 - f1: 0.6029 - val_loss: 0.1131 - val_f1: 0.5894\n",
      "Epoch 5/20\n",
      "1279998/1279998 [==============================] - 186s 145us/step - loss: 0.1138 - f1: 0.6173 - val_loss: 0.1126 - val_f1: 0.6059\n",
      "Epoch 6/20\n",
      "1279998/1279998 [==============================] - 184s 144us/step - loss: 0.1121 - f1: 0.6317 - val_loss: 0.1141 - val_f1: 0.5857\n",
      "Epoch 7/20\n",
      "1279998/1279998 [==============================] - 191s 149us/step - loss: 0.1116 - f1: 0.6355 - val_loss: 0.1107 - val_f1: 0.6473\n",
      "Epoch 8/20\n",
      "1279998/1279998 [==============================] - 192s 150us/step - loss: 0.1108 - f1: 0.6423 - val_loss: 0.1104 - val_f1: 0.6462\n",
      "Epoch 9/20\n",
      "1279998/1279998 [==============================] - 185s 145us/step - loss: 0.1105 - f1: 0.6485 - val_loss: 0.1111 - val_f1: 0.6579\n",
      "Epoch 10/20\n",
      "1279998/1279998 [==============================] - 182s 142us/step - loss: 0.1099 - f1: 0.6531 - val_loss: 0.1108 - val_f1: 0.6459\n",
      "Epoch 11/20\n",
      "1279998/1279998 [==============================] - 181s 142us/step - loss: 0.1076 - f1: 0.6600 - val_loss: 0.1097 - val_f1: 0.6463\n",
      "Epoch 12/20\n",
      "1279998/1279998 [==============================] - 182s 142us/step - loss: 0.1052 - f1: 0.6652 - val_loss: 0.1103 - val_f1: 0.6044\n",
      "Epoch 13/20\n",
      "1279998/1279998 [==============================] - 182s 142us/step - loss: 0.1028 - f1: 0.6667 - val_loss: 0.1070 - val_f1: 0.6585\n",
      "Epoch 14/20\n",
      "1279998/1279998 [==============================] - 183s 143us/step - loss: 0.1006 - f1: 0.6717 - val_loss: 0.1066 - val_f1: 0.6601\n",
      "Epoch 15/20\n",
      "1279998/1279998 [==============================] - 192s 150us/step - loss: 0.0986 - f1: 0.6748 - val_loss: 0.1038 - val_f1: 0.6701\n",
      "Epoch 16/20\n",
      "1279998/1279998 [==============================] - 191s 149us/step - loss: 0.0966 - f1: 0.6788 - val_loss: 0.1031 - val_f1: 0.6701\n",
      "Epoch 17/20\n",
      "1279998/1279998 [==============================] - 193s 151us/step - loss: 0.0951 - f1: 0.6808 - val_loss: 0.1035 - val_f1: 0.6277\n",
      "Epoch 18/20\n",
      "1279998/1279998 [==============================] - 192s 150us/step - loss: 0.0933 - f1: 0.6826 - val_loss: 0.1018 - val_f1: 0.6480\n",
      "Epoch 19/20\n",
      "1279998/1279998 [==============================] - 195s 152us/step - loss: 0.0917 - f1: 0.6856 - val_loss: 0.1003 - val_f1: 0.6622\n",
      "Epoch 20/20\n",
      "1279998/1279998 [==============================] - 197s 154us/step - loss: 0.0909 - f1: 0.6894 - val_loss: 0.1010 - val_f1: 0.6689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f4536a3c8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1h 2min 52s\n"
     ]
    }
   ],
   "source": [
    "clr = CyclicLR(base_lr=0.001, max_lr=0.008,\n",
    "               step_size=3000., mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "model = model_lstm_atten(embedding_matrix)\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n",
    "          class_weight='auto',\n",
    "          epochs=20, batch_size=4096,\n",
    "          callbacks= [clr]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "deep_train_preds = model.predict(X_train, batch_size=4096)\n",
    "tfidf_train_preds = ngram_model.predict(tfidf_trainx, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.1 ms\n"
     ]
    }
   ],
   "source": [
    "combined_preds = np.concatenate([deep_train_preds, tfidf_train_preds], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7246832878636449"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# clf = KNeighborsClassifier(n_neighbors=10)\n",
    "# clf = DecisionTreeClassifier(random_state=2019, max_depth=4)\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "# clf = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(random_state=2019, max_depth=4), n_estimators=20)\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=2019, n_jobs=-1)\n",
    "clf.fit(combined_preds, Y_train)\n",
    "f1_score(Y_train, clf.predict(combined_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "deep_val_preds = model.predict(X_val, batch_size=4096)\n",
    "tfidf_val_preds = ngram_model.predict(tfidf_valx, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[75,26124,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_1/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _class=[\"loc:@bidirectional_1/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](spatial_dropout1d_1/cond/Merge, bidirectional_2/transpose/perm)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-f735f55d4cbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m get_lstm_emb = K.function([model.layers[0].input],\n\u001b[1;32m      4\u001b[0m                                   [model.layers[-3].output])\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lstm_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/persona/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[75,26124,300] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional_1/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _class=[\"loc:@bidirectional_1/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](spatial_dropout1d_1/cond/Merge, bidirectional_2/transpose/perm)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "get_lstm_emb = K.function([model.layers[0].input],\n",
    "                                  [model.layers[-3].output])\n",
    "layer_output = get_lstm_emb([X_val])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(layer_output[:500])\n",
    "# X_embedded = TSNE(n_components=2).fit_transform(tfidf_valx[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X_embedded[:,0], X_embedded[:,1], c=Y_val[:500], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "combined_preds_val = np.concatenate([deep_val_preds, tfidf_val_preds], axis=-1)\n",
    "val_preds = clf.predict_proba(combined_preds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37 0.6798307475317349\n",
      "time: 584 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paragag/persona/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_threshold, best_score = 0.5, 0\n",
    "for i in range(100):\n",
    "    score = f1_score(Y_val, (val_preds[:,1]  > (i/100)) * 1)\n",
    "    if score > best_score:\n",
    "        best_threshold = i/100\n",
    "        best_score = score\n",
    "print(best_threshold, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.16 s\n"
     ]
    }
   ],
   "source": [
    "# Use best threshold to construct predictions\n",
    "deep_test_preds = model.predict(test_X, batch_size=4096)\n",
    "tfidf_test_preds = ngram_model.predict(tfidf_testX, batch_size=2048)\n",
    "\n",
    "combined_preds_test = np.concatenate([deep_test_preds, tfidf_test_preds], axis=-1)\n",
    "test_preds = clf.predict_proba(combined_preds_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_predictions = (test_preds > best_threshold).astype(int)\n",
    "\n",
    "test_df = pd.read_csv(base_dir +\"/test.csv\", usecols=[\"qid\"])\n",
    "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "out_df['prediction'] = needed_predictions\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
