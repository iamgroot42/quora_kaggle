{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings', 'train.csv', 'test.csv', 'sample_submission.csv']\n",
      "['paragram_300_sl999', 'glove.840B.300d', 'GoogleNews-vectors-negative300', 'wiki-news-300d-1M']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential, Model # initialize neural network library\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate, average\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import time\n",
    "import os\n",
    "base_dir = \"./input\"\n",
    "print(os.listdir(base_dir))\n",
    "print(os.listdir(base_dir + \"/embeddings\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import KeyedVectors as kv\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't hog GPU\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "def get_google_embeddings():\n",
    "    emb_path = base_dir + '/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "    obj = kv.load_word2vec_format(emb_path, binary=True)\n",
    "    return (obj)\n",
    "embedding_obj = get_google_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train datasets shape: (1306122, 3)\n",
      "Test datasets shape: (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "# load train and test datasets\n",
    "train_df = pd.read_csv(base_dir+ \"/train.csv\")\n",
    "test_df = pd.read_csv(base_dir + \"/test.csv\")\n",
    "print(\"Train datasets shape:\", train_df.shape)\n",
    "print(\"Test datasets shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:04<00:00, 304355.52it/s]\n",
      "100%|██████████| 1306122/1306122 [00:05<00:00, 229688.16it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_df[\"question_text\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 508823/508823 [00:02<00:00, 202787.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 24.31% of vocab\n",
      "Found embeddings for  78.75% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, \" & \")\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:15<00:00, 81730.33it/s] \n",
      "100%|██████████| 56370/56370 [00:00<00:00, 71444.93it/s]\n",
      "100%|██████████| 1306122/1306122 [00:05<00:00, 234922.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "\n",
    "sentences = train_df[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253623/253623 [00:01<00:00, 155634.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 57.38% of vocab\n",
      "Found embeddings for  89.99% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:19<00:00, 66068.13it/s]\n",
      "100%|██████████| 56370/56370 [00:00<00:00, 66853.15it/s]\n",
      "100%|██████████| 1306122/1306122 [00:06<00:00, 213848.75it/s]\n",
      "100%|██████████| 1306122/1306122 [00:07<00:00, 183791.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "\n",
    "sentences = train_df[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 242997/242997 [00:01<00:00, 157093.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.41% of vocab\n",
      "Found embeddings for  90.75% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'favour': 'favor',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium',\n",
    "                'Snapchat': 'social medium',\n",
    "                'wasnt': 'was not',\n",
    "                'Whatis': 'what is',\n",
    "                'hasnt': 'has not',\n",
    "                'practise': 'practice',\n",
    "                'programme': 'program',\n",
    "                'behaviour': 'behavior',\n",
    "                'travelled': 'traveled',\n",
    "                'licence': 'license',\n",
    "                'defence': 'defense',\n",
    "                'modelling': 'modeling',\n",
    "                'recognise': 'recognize',\n",
    "                'Isnt': 'is not',\n",
    "                'demonetisation': 'demonetization',\n",
    "                'analyse': 'analyze',\n",
    "                'programrs': 'programers',\n",
    "                'programr': 'programer',\n",
    "                'realise': 'realize',\n",
    "                'honours': 'honors',\n",
    "                'neighbour': 'neighbor',\n",
    "                'jewellery': 'jewelery'\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:15<00:00, 84382.21it/s] \n",
      "100%|██████████| 56370/56370 [00:00<00:00, 79072.44it/s] \n",
      "100%|██████████| 1306122/1306122 [00:06<00:00, 215034.61it/s]\n",
      "100%|██████████| 1306122/1306122 [00:07<00:00, 182258.92it/s]\n",
      "100%|██████████| 1306122/1306122 [00:04<00:00, 286067.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "sentences = train_df[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 242868/242868 [00:01<00:00, 189661.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 60.44% of vocab\n",
      "Found embeddings for  98.99% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab, embedding_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 #embeddings.shape[1] # how big is each word vector\n",
    "max_features = len(vocab.keys()) + 2\n",
    "maxlen = 100 # max number of words in a question to use\n",
    "\n",
    "UNK_TOKEN = max_features + 1\n",
    "END_TOKEN = max_features + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill up the missing values\n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data, vocab):\n",
    "    new_data = []\n",
    "\n",
    "    def get_word(word):\n",
    "        word_id = UNK_TOKEN\n",
    "        if word in vocab:\n",
    "            word_id = vocab.get(word)\n",
    "        return word_id\n",
    "\n",
    "    for row in data:\n",
    "        new_data.append([get_word(word) for word in row])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the sentences\n",
    "# tokenizer = Tokenizer(num_words=max_features)\n",
    "# tokenizer.fit_on_texts(list(train_X))\n",
    "# train_X = tokenizer.texts_to_sequences(train_X)\n",
    "# test_X = tokenizer.texts_to_sequences(test_X)\n",
    "words_to_use = list(vocab.keys())\n",
    "vocab_to_use = {words_to_use[i]:i for i in range(len(words_to_use))}\n",
    "train_X = tokenize_data(train_X, vocab_to_use)\n",
    "test_X = tokenize_data(test_X, vocab_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(data_vocab.keys())[:5])\n",
    "# print(data_vocab.get('Accountabilities').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen, padding='post', truncating='post', value=END_TOKEN)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen, padding='post', truncating='post', value=END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the target values\n",
    "train_y = train_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F-1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_embedding_matrix(tokenize_vocab):\n",
    "    emb = np.zeros((max_features, embed_size))\n",
    "    for key, value in tokenize_vocab.items():\n",
    "        if key in embedding_obj.wv:\n",
    "            emb[value] = embedding_obj.wv[key].shape\n",
    "    emb[max_features - 2] = np.random.rand(300,)\n",
    "    emb[max_features - 1] = np.random.rand(300,)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paragag/persona/lib/python3.5/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/home/paragag/persona/lib/python3.5/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = construct_embedding_matrix(vocab_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert max_features == embedding_matrix.shape[0]\n",
    "assert embed_size == embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_nn_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, trainable=False, weights=[embedding_matrix])(inp)\n",
    "    x = LSTM(128, return_sequences=True)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPool1D()(x)\n",
    "    conc = average([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[f1_score])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain with 50% of the minimum class, balanced\n",
    "y_one = np.where(train_y==1)[0]\n",
    "y_zero = np.where(train_y==0)[0]\n",
    "n_initial = int(0.50 * min(len(y_one), len(y_zero)))\n",
    "initial_idx = y_one[:n_initial] + y_zero[:n_initial] \n",
    "X_initial = train_X[initial_idx]\n",
    "y_initial = train_y[initial_idx]\n",
    "\n",
    "# generate the pool\n",
    "# remove the initial data from the training dataset\n",
    "X_pool = np.delete(train_X, initial_idx, axis=0)\n",
    "y_pool = np.delete(train_y, initial_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 300)     72861000    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 100, 128)     219648      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100, 128)     0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_4 (Average)             (None, 128)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            129         average_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 73,080,777\n",
      "Trainable params: 219,777\n",
      "Non-trainable params: 72,861,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 36364 samples, validate on 4041 samples\n",
      "Epoch 1/2\n",
      "36364/36364 [==============================] - 9s 248us/step - loss: 1.2325 - f1_score: 0.1960 - val_loss: 1.2275 - val_f1_score: 0.1872\n",
      "Epoch 2/2\n",
      "36364/36364 [==============================] - 7s 195us/step - loss: 1.2130 - f1_score: 0.2023 - val_loss: 1.2277 - val_f1_score: 0.1849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ef8583f37f0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-train model\n",
    "model = second_nn_model()\n",
    "\n",
    "class_weight = {0: 1, 1:np.sum(y_initial == 0)/np.sum(y_initial == 1)}\n",
    "model.fit(X_initial, y_initial, epochs=2, batch_size=1024, validation_split=0.1, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1\n",
      "1265717/1265717 [==============================] - 24s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 197us/step - loss: 1.4394 - f1_score: 0.1303 - val_loss: 1.4762 - val_f1_score: 0.1152\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 198us/step - loss: 1.4315 - f1_score: 0.1324 - val_loss: 1.4735 - val_f1_score: 0.1152\n",
      "\n",
      "Iteration 2\n",
      "1218803/1218803 [==============================] - 23s 18us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 201us/step - loss: 1.2280 - f1_score: 0.1666 - val_loss: 1.1393 - val_f1_score: 0.0980\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 197us/step - loss: 1.2054 - f1_score: 0.1701 - val_loss: 1.1114 - val_f1_score: 0.0970\n",
      "\n",
      "Iteration 3\n",
      "1170483/1170483 [==============================] - 21s 18us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 197us/step - loss: 1.4114 - f1_score: 0.1476 - val_loss: 1.5023 - val_f1_score: 0.1249\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 201us/step - loss: 1.3931 - f1_score: 0.1525 - val_loss: 1.4973 - val_f1_score: 0.1268\n",
      "\n",
      "Iteration 4\n",
      "1123731/1123731 [==============================] - 22s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 19s 238us/step - loss: 1.1414 - f1_score: 0.1755 - val_loss: 0.9984 - val_f1_score: 0.0663\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 20s 242us/step - loss: 1.1323 - f1_score: 0.1701 - val_loss: 1.0115 - val_f1_score: 0.0647\n",
      "\n",
      "Iteration 5\n",
      "1075121/1075121 [==============================] - 20s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 18s 218us/step - loss: 1.2441 - f1_score: 0.1530 - val_loss: 1.2607 - val_f1_score: 0.1003\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 20s 240us/step - loss: 1.2392 - f1_score: 0.1591 - val_loss: 1.2577 - val_f1_score: 0.1022\n",
      "\n",
      "Iteration 6\n",
      "1027161/1027161 [==============================] - 20s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 17s 214us/step - loss: 1.3940 - f1_score: 0.1455 - val_loss: 1.4745 - val_f1_score: 0.1200\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 18s 221us/step - loss: 1.3841 - f1_score: 0.1487 - val_loss: 1.4699 - val_f1_score: 0.1191\n",
      "\n",
      "Iteration 7\n",
      "980300/980300 [==============================] - 19s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 244us/step - loss: 1.2715 - f1_score: 0.1469 - val_loss: 1.2201 - val_f1_score: 0.0788\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 233us/step - loss: 1.2677 - f1_score: 0.1495 - val_loss: 1.2150 - val_f1_score: 0.0764\n",
      "\n",
      "Iteration 8\n",
      "932457/932457 [==============================] - 18s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 246us/step - loss: 1.2210 - f1_score: 0.1541 - val_loss: 1.1920 - val_f1_score: 0.0783\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 235us/step - loss: 1.2145 - f1_score: 0.1607 - val_loss: 1.1993 - val_f1_score: 0.0810\n",
      "\n",
      "Iteration 9\n",
      "884328/884328 [==============================] - 17s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 240us/step - loss: 1.2819 - f1_score: 0.1480 - val_loss: 1.3541 - val_f1_score: 0.1115\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 20s 240us/step - loss: 1.2841 - f1_score: 0.1488 - val_loss: 1.3456 - val_f1_score: 0.1041\n",
      "\n",
      "Iteration 10\n",
      "836697/836697 [==============================] - 16s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 19s 238us/step - loss: 1.2164 - f1_score: 0.1578 - val_loss: 1.1636 - val_f1_score: nan\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 20s 247us/step - loss: 1.2138 - f1_score: 0.1576 - val_loss: 1.1645 - val_f1_score: nan\n",
      "\n",
      "Iteration 11\n",
      "788565/788565 [==============================] - 15s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 241us/step - loss: 1.3196 - f1_score: 0.1416 - val_loss: 1.3662 - val_f1_score: 0.1028\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 232us/step - loss: 1.3163 - f1_score: 0.1414 - val_loss: 1.3737 - val_f1_score: 0.0983\n",
      "\n",
      "Iteration 12\n",
      "741185/741185 [==============================] - 14s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 245us/step - loss: 1.2113 - f1_score: 0.1603 - val_loss: 1.1605 - val_f1_score: 0.0714\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 237us/step - loss: 1.2107 - f1_score: 0.1608 - val_loss: 1.1627 - val_f1_score: 0.0735\n",
      "\n",
      "Iteration 13\n",
      "693049/693049 [==============================] - 13s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 240us/step - loss: 1.2632 - f1_score: 0.1496 - val_loss: 1.2824 - val_f1_score: 0.0937\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 20s 241us/step - loss: 1.2577 - f1_score: 0.1465 - val_loss: 1.2779 - val_f1_score: 0.0941\n",
      "\n",
      "Iteration 14\n",
      "645261/645261 [==============================] - 12s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 240us/step - loss: 1.2960 - f1_score: 0.1449 - val_loss: 1.3214 - val_f1_score: 0.0938\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 238us/step - loss: 1.2932 - f1_score: 0.1464 - val_loss: 1.3238 - val_f1_score: 0.0948\n",
      "\n",
      "Iteration 15\n",
      "597729/597729 [==============================] - 12s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 20s 240us/step - loss: 1.2399 - f1_score: 0.1495 - val_loss: 1.2306 - val_f1_score: 0.0952\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 239us/step - loss: 1.2400 - f1_score: 0.1508 - val_loss: 1.2289 - val_f1_score: 0.1002\n",
      "\n",
      "Iteration 16\n",
      "549830/549830 [==============================] - 11s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 19s 235us/step - loss: 1.2507 - f1_score: 0.1490 - val_loss: 1.2731 - val_f1_score: 0.0998\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 239us/step - loss: 1.2480 - f1_score: 0.1540 - val_loss: 1.2780 - val_f1_score: 0.0929\n",
      "\n",
      "Iteration 17\n",
      "502001/502001 [==============================] - 10s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 19s 236us/step - loss: 1.3406 - f1_score: 0.1449 - val_loss: 1.4464 - val_f1_score: 0.1097\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 19s 228us/step - loss: 1.3381 - f1_score: 0.1447 - val_loss: 1.4415 - val_f1_score: 0.1099\n",
      "\n",
      "Iteration 18\n",
      "454882/454882 [==============================] - 9s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 19s 235us/step - loss: 1.2126 - f1_score: 0.1582 - val_loss: 1.1591 - val_f1_score: 0.0786\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 17s 215us/step - loss: 1.2124 - f1_score: 0.1610 - val_loss: 1.1764 - val_f1_score: 0.0755\n",
      "\n",
      "Iteration 19\n",
      "406764/406764 [==============================] - 7s 18us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 193us/step - loss: 1.2509 - f1_score: 0.1475 - val_loss: 1.2388 - val_f1_score: 0.0968\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 192us/step - loss: 1.2505 - f1_score: 0.1508 - val_loss: 1.2384 - val_f1_score: 0.0966\n",
      "\n",
      "Iteration 20\n",
      "358923/358923 [==============================] - 7s 18us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 191us/step - loss: 1.2703 - f1_score: 0.1447 - val_loss: 1.3364 - val_f1_score: 0.1043\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 196us/step - loss: 1.2690 - f1_score: 0.1486 - val_loss: 1.3338 - val_f1_score: 0.1099\n",
      "\n",
      "Iteration 21\n",
      "311260/311260 [==============================] - 6s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 195us/step - loss: 1.2510 - f1_score: 0.1499 - val_loss: 1.2820 - val_f1_score: 0.1008\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 192us/step - loss: 1.2508 - f1_score: 0.1495 - val_loss: 1.2848 - val_f1_score: 0.1018\n",
      "\n",
      "Iteration 22\n",
      "263452/263452 [==============================] - 5s 18us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 193us/step - loss: 1.2429 - f1_score: 0.1502 - val_loss: 1.2320 - val_f1_score: 0.1031\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 192us/step - loss: 1.2405 - f1_score: 0.1532 - val_loss: 1.2320 - val_f1_score: 0.1014\n",
      "\n",
      "Iteration 23\n",
      "215576/215576 [==============================] - 4s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 192us/step - loss: 1.2703 - f1_score: 0.1495 - val_loss: 1.4082 - val_f1_score: 0.1046\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 191us/step - loss: 1.2729 - f1_score: 0.1456 - val_loss: 1.3849 - val_f1_score: 0.1148\n",
      "\n",
      "Iteration 24\n",
      "167961/167961 [==============================] - 3s 18us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 192us/step - loss: 1.2591 - f1_score: 0.1488 - val_loss: 1.3735 - val_f1_score: 0.1210\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 196us/step - loss: 1.2610 - f1_score: 0.1449 - val_loss: 1.3736 - val_f1_score: 0.1209\n",
      "\n",
      "Iteration 25\n",
      "120323/120323 [==============================] - 2s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 197us/step - loss: 1.2917 - f1_score: 0.1469 - val_loss: 1.4071 - val_f1_score: 0.1412\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 201us/step - loss: 1.2895 - f1_score: 0.1455 - val_loss: 1.4067 - val_f1_score: 0.1412\n",
      "\n",
      "Iteration 26\n",
      "72916/72916 [==============================] - 1s 19us/step\n",
      "Train on 81364 samples, validate on 9041 samples\n",
      "Epoch 1/2\n",
      "81364/81364 [==============================] - 16s 200us/step - loss: 1.2481 - f1_score: 0.1555 - val_loss: 1.0184 - val_f1_score: 0.1772\n",
      "Epoch 2/2\n",
      "81364/81364 [==============================] - 16s 196us/step - loss: 1.2455 - f1_score: 0.1567 - val_loss: 1.0362 - val_f1_score: 0.1772\n",
      "\n",
      "Iteration 27\n",
      "25218/25218 [==============================] - 0s 20us/step\n",
      "Train on 51768 samples, validate on 5753 samples\n",
      "Epoch 1/2\n",
      "51768/51768 [==============================] - 10s 198us/step - loss: 1.2273 - f1_score: 0.1742 - val_loss: 1.1414 - val_f1_score: 0.1543\n",
      "Epoch 2/2\n",
      "51768/51768 [==============================] - 10s 200us/step - loss: 1.2228 - f1_score: 0.1776 - val_loss: 1.1460 - val_f1_score: 0.1543\n",
      "\n",
      "Iteration 28\n",
      "8843/8843 [==============================] - 0s 21us/step\n",
      "Train on 41571 samples, validate on 4620 samples\n",
      "Epoch 1/2\n",
      "41571/41571 [==============================] - 8s 203us/step - loss: 1.2288 - f1_score: 0.1934 - val_loss: 1.2499 - val_f1_score: 0.1454\n",
      "Epoch 2/2\n",
      "41571/41571 [==============================] - 8s 202us/step - loss: 1.2245 - f1_score: 0.1913 - val_loss: 1.2512 - val_f1_score: 0.1461\n",
      "\n",
      "Iteration 29\n",
      "3335/3335 [==============================] - 0s 26us/step\n",
      "Train on 38118 samples, validate on 4236 samples\n",
      "Epoch 1/2\n",
      "38118/38118 [==============================] - 8s 216us/step - loss: 1.2258 - f1_score: 0.1934 - val_loss: 1.3606 - val_f1_score: 0.1661\n",
      "Epoch 2/2\n",
      "38118/38118 [==============================] - 8s 216us/step - loss: 1.2194 - f1_score: 0.1927 - val_loss: 1.3605 - val_f1_score: 0.1646\n",
      "\n",
      "Iteration 30\n",
      "1503/1503 [==============================] - 0s 48us/step\n"
     ]
    }
   ],
   "source": [
    "# Per epoch, use active learning-based sampling for over-represented class\n",
    "epochs = 50\n",
    "confusion_range = (0.3, 0.7)\n",
    "for i in range(epochs - 1):\n",
    "    try:\n",
    "        print(\"\\nIteration %d\" % (i + 1))\n",
    "        y_pred = np.squeeze(model.predict(X_pool, batch_size=16384, verbose=1))\n",
    "        above_threshold = np.where(y_pred >= confusion_range[0])[0]\n",
    "        below_threshold = np.where(y_pred <= confusion_range[1])[0]\n",
    "        selected_indices = list(set(above_threshold) & set(below_threshold))\n",
    "        sorted_indices = sorted(selected_indices, key=lambda x: max(confusion_range[1] - y_pred[x], y_pred[x]-confusion_range[0]))\n",
    "    \n",
    "        # Pick corresponding data\n",
    "        y_use = y_pool[sorted_indices[:50000]]\n",
    "        x_use = X_pool[sorted_indices[:50000]]\n",
    "        \n",
    "        # Stop when done with pool data\n",
    "        if(len(y_use) < 1000):\n",
    "            break\n",
    "    \n",
    "        # Mix in randomly sampled data from initial data (to avoide concept drift)\n",
    "        indices = np.random.permutation(len(X_initial))[:50000]\n",
    "        x_use_mixed = np.concatenate([X_initial[indices], x_use])\n",
    "        y_use_mixed = np.concatenate([y_initial[indices], y_use])\n",
    "        #class_weight = {0: 1, 1:np.sum(y_use_mixed == 0)/np.sum(y_use_mixed == 1)}\n",
    "        class_weight = {0: 1, 1:20}\n",
    "\n",
    "        # Fit for this epoch\n",
    "        model.fit(x_use_mixed, y_use_mixed, epochs=2, batch_size=1024, validation_split=0.1, class_weight=class_weight)\n",
    "    \n",
    "        # Dump out examples over-represented class\n",
    "        over_represented_indices = np.where(y_use==0)[0]\n",
    "        X_pool = np.delete(X_pool, over_represented_indices, axis=0)\n",
    "        y_pool = np.delete(y_pool, over_represented_indices, axis=0)\n",
    "    except:\n",
    "        break\n",
    "        print(\"Active sampling done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from modAL.models import ActiveLearner\n",
    "\n",
    "\n",
    "# # create the classifier\n",
    "# classifier = KerasClassifier(second_nn_model)\n",
    "\n",
    "# # initialize ActiveLearner\n",
    "# learner = ActiveLearner(\n",
    "#     estimator=classifier,\n",
    "#     X_training=X_initial, y_training=y_initial,\n",
    "#     batch_size=1024, class_weight=class_weight,\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_queries = 5\n",
    "# for idx in range(n_queries):\n",
    "#     print('Query no. %d' % (idx + 1))\n",
    "#     query_idx, query_instance = learner.query(X_pool, n_instances=5000, verbose=1)\n",
    "#     learner.teach(\n",
    "#         X=X_pool[query_idx], y=y_pool[query_idx], only_new=True, class_weight=class_weight,\n",
    "#         verbose=1, batch_size=1024\n",
    "#     )\n",
    "#     # remove queried instance from pool\n",
    "#     X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "#     y_pool = np.delete(y_pool, query_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create model, train and predict\n",
    "# class_weight = {0: 1, 1:np.sum(train_y == 0)/np.sum(train_y == 1)}\n",
    "\n",
    "# model.fit(train_X, train_y, epochs=5, validation_split=0.1, class_weight=class_weight, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_test_y = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noemb_test_y.shape\n",
    "y_pred_test = (pred_noemb_test_y > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect garbage\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission\n",
    "import time\n",
    "sub_df = pd.DataFrame({'qid':test_df.qid.values})\n",
    "sub_df['prediction'] = y_pred_test\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quora",
   "language": "python",
   "name": "quora"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
